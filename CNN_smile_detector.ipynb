{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_data = torchvision.datasets.ImageFolder(r'D:\\PycharmProjects\\NNs\\CNN\\datasets\\train_folder', transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(r'D:\\PycharmProjects\\NNs\\CNN\\datasets\\test_folder', transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.l = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = AlexNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.005, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def fit(train_dataset: torch.Tensor, n_epochs=20):\n",
    "    for epoch in range(n_epochs):\n",
    "        pred_per_epoch = []\n",
    "        true = []\n",
    "        for i, batch in enumerate(train_dataset):\n",
    "            inputs, target = batch\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            true += target.tolist()\n",
    "            pred_per_epoch += predicted.tolist()\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, n_epochs, i + 1, len(train_dataset), loss.item()))\n",
    "        print(\"Accuracy at {} epoch: {:.2f}%\".format(epoch + 1, accuracy_score(true, pred_per_epoch)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def score(dataset: torch.Tensor):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for inputs, target in dataset:\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        predict = model(inputs)\n",
    "        _, predicted = torch.max(predict.data, 1)\n",
    "        y_true += target.tolist()\n",
    "        y_pred += predicted.tolist()\n",
    "\n",
    "    print(\"Accuracy: {}%. ROC-AUC: {:.2f}\".format(accuracy_score(y_true, y_pred), roc_auc_score(y_true, y_pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    target = ['smile', 'not smile']\n",
    "    img = Image.open(path)\n",
    "    img = transform(img).to(device)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "\n",
    "    model.eval()\n",
    "    prediction = torch.max(model(img), 1).indices.item()\n",
    "    print(target[prediction])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/88], Loss: 0.6578\n",
      "Epoch [1/20], Step [2/88], Loss: 0.7792\n",
      "Epoch [1/20], Step [3/88], Loss: 0.6998\n",
      "Epoch [1/20], Step [4/88], Loss: 0.6740\n",
      "Epoch [1/20], Step [5/88], Loss: 0.6642\n",
      "Epoch [1/20], Step [6/88], Loss: 0.6990\n",
      "Epoch [1/20], Step [7/88], Loss: 0.6978\n",
      "Epoch [1/20], Step [8/88], Loss: 0.6774\n",
      "Epoch [1/20], Step [9/88], Loss: 0.6774\n",
      "Epoch [1/20], Step [10/88], Loss: 0.7389\n",
      "Epoch [1/20], Step [11/88], Loss: 0.6685\n",
      "Epoch [1/20], Step [12/88], Loss: 0.6763\n",
      "Epoch [1/20], Step [13/88], Loss: 0.6731\n",
      "Epoch [1/20], Step [14/88], Loss: 0.7107\n",
      "Epoch [1/20], Step [15/88], Loss: 0.7001\n",
      "Epoch [1/20], Step [16/88], Loss: 0.6470\n",
      "Epoch [1/20], Step [17/88], Loss: 0.7621\n",
      "Epoch [1/20], Step [18/88], Loss: 0.6863\n",
      "Epoch [1/20], Step [19/88], Loss: 0.6671\n",
      "Epoch [1/20], Step [20/88], Loss: 0.6738\n",
      "Epoch [1/20], Step [21/88], Loss: 0.6664\n",
      "Epoch [1/20], Step [22/88], Loss: 0.6933\n",
      "Epoch [1/20], Step [23/88], Loss: 0.6442\n",
      "Epoch [1/20], Step [24/88], Loss: 0.7033\n",
      "Epoch [1/20], Step [25/88], Loss: 0.6256\n",
      "Epoch [1/20], Step [26/88], Loss: 0.6630\n",
      "Epoch [1/20], Step [27/88], Loss: 0.6656\n",
      "Epoch [1/20], Step [28/88], Loss: 0.6765\n",
      "Epoch [1/20], Step [29/88], Loss: 0.6241\n",
      "Epoch [1/20], Step [30/88], Loss: 0.6606\n",
      "Epoch [1/20], Step [31/88], Loss: 0.8379\n",
      "Epoch [1/20], Step [32/88], Loss: 0.5815\n",
      "Epoch [1/20], Step [33/88], Loss: 0.5946\n",
      "Epoch [1/20], Step [34/88], Loss: 0.6974\n",
      "Epoch [1/20], Step [35/88], Loss: 0.6527\n",
      "Epoch [1/20], Step [36/88], Loss: 0.6383\n",
      "Epoch [1/20], Step [37/88], Loss: 0.5942\n",
      "Epoch [1/20], Step [38/88], Loss: 0.7599\n",
      "Epoch [1/20], Step [39/88], Loss: 0.7475\n",
      "Epoch [1/20], Step [40/88], Loss: 0.6795\n",
      "Epoch [1/20], Step [41/88], Loss: 0.6629\n",
      "Epoch [1/20], Step [42/88], Loss: 0.6675\n",
      "Epoch [1/20], Step [43/88], Loss: 0.6125\n",
      "Epoch [1/20], Step [44/88], Loss: 0.6242\n",
      "Epoch [1/20], Step [45/88], Loss: 0.6732\n",
      "Epoch [1/20], Step [46/88], Loss: 0.6339\n",
      "Epoch [1/20], Step [47/88], Loss: 0.6430\n",
      "Epoch [1/20], Step [48/88], Loss: 0.5990\n",
      "Epoch [1/20], Step [49/88], Loss: 0.5753\n",
      "Epoch [1/20], Step [50/88], Loss: 0.5894\n",
      "Epoch [1/20], Step [51/88], Loss: 0.6301\n",
      "Epoch [1/20], Step [52/88], Loss: 0.5272\n",
      "Epoch [1/20], Step [53/88], Loss: 0.6345\n",
      "Epoch [1/20], Step [54/88], Loss: 0.6443\n",
      "Epoch [1/20], Step [55/88], Loss: 0.6859\n",
      "Epoch [1/20], Step [56/88], Loss: 0.5584\n",
      "Epoch [1/20], Step [57/88], Loss: 0.6257\n",
      "Epoch [1/20], Step [58/88], Loss: 0.6136\n",
      "Epoch [1/20], Step [59/88], Loss: 0.5461\n",
      "Epoch [1/20], Step [60/88], Loss: 0.6114\n",
      "Epoch [1/20], Step [61/88], Loss: 0.6468\n",
      "Epoch [1/20], Step [62/88], Loss: 0.6092\n",
      "Epoch [1/20], Step [63/88], Loss: 0.6608\n",
      "Epoch [1/20], Step [64/88], Loss: 0.5861\n",
      "Epoch [1/20], Step [65/88], Loss: 0.6188\n",
      "Epoch [1/20], Step [66/88], Loss: 0.5758\n",
      "Epoch [1/20], Step [67/88], Loss: 0.6149\n",
      "Epoch [1/20], Step [68/88], Loss: 0.6834\n",
      "Epoch [1/20], Step [69/88], Loss: 0.5597\n",
      "Epoch [1/20], Step [70/88], Loss: 0.6484\n",
      "Epoch [1/20], Step [71/88], Loss: 0.5186\n",
      "Epoch [1/20], Step [72/88], Loss: 0.6533\n",
      "Epoch [1/20], Step [73/88], Loss: 0.5520\n",
      "Epoch [1/20], Step [74/88], Loss: 0.5867\n",
      "Epoch [1/20], Step [75/88], Loss: 0.6664\n",
      "Epoch [1/20], Step [76/88], Loss: 0.5966\n",
      "Epoch [1/20], Step [77/88], Loss: 0.6898\n",
      "Epoch [1/20], Step [78/88], Loss: 0.6759\n",
      "Epoch [1/20], Step [79/88], Loss: 0.5944\n",
      "Epoch [1/20], Step [80/88], Loss: 0.5946\n",
      "Epoch [1/20], Step [81/88], Loss: 0.5396\n",
      "Epoch [1/20], Step [82/88], Loss: 0.5144\n",
      "Epoch [1/20], Step [83/88], Loss: 0.5287\n",
      "Epoch [1/20], Step [84/88], Loss: 0.5977\n",
      "Epoch [1/20], Step [85/88], Loss: 0.5303\n",
      "Epoch [1/20], Step [86/88], Loss: 0.5390\n",
      "Epoch [1/20], Step [87/88], Loss: 0.7317\n",
      "Epoch [1/20], Step [88/88], Loss: 0.4162\n",
      "Accuracy at 1 epoch: 0.62%\n",
      "Epoch [2/20], Step [1/88], Loss: 0.6286\n",
      "Epoch [2/20], Step [2/88], Loss: 0.5915\n",
      "Epoch [2/20], Step [3/88], Loss: 0.4997\n",
      "Epoch [2/20], Step [4/88], Loss: 0.5800\n",
      "Epoch [2/20], Step [5/88], Loss: 0.3735\n",
      "Epoch [2/20], Step [6/88], Loss: 0.4478\n",
      "Epoch [2/20], Step [7/88], Loss: 0.5706\n",
      "Epoch [2/20], Step [8/88], Loss: 0.5461\n",
      "Epoch [2/20], Step [9/88], Loss: 0.5249\n",
      "Epoch [2/20], Step [10/88], Loss: 0.3503\n",
      "Epoch [2/20], Step [11/88], Loss: 0.6500\n",
      "Epoch [2/20], Step [12/88], Loss: 0.4739\n",
      "Epoch [2/20], Step [13/88], Loss: 0.4726\n",
      "Epoch [2/20], Step [14/88], Loss: 0.4933\n",
      "Epoch [2/20], Step [15/88], Loss: 0.4984\n",
      "Epoch [2/20], Step [16/88], Loss: 0.5010\n",
      "Epoch [2/20], Step [17/88], Loss: 0.5044\n",
      "Epoch [2/20], Step [18/88], Loss: 0.4409\n",
      "Epoch [2/20], Step [19/88], Loss: 0.5330\n",
      "Epoch [2/20], Step [20/88], Loss: 0.4738\n",
      "Epoch [2/20], Step [21/88], Loss: 0.5827\n",
      "Epoch [2/20], Step [22/88], Loss: 0.5135\n",
      "Epoch [2/20], Step [23/88], Loss: 0.3974\n",
      "Epoch [2/20], Step [24/88], Loss: 0.3267\n",
      "Epoch [2/20], Step [25/88], Loss: 0.4828\n",
      "Epoch [2/20], Step [26/88], Loss: 0.4096\n",
      "Epoch [2/20], Step [27/88], Loss: 0.4896\n",
      "Epoch [2/20], Step [28/88], Loss: 0.3665\n",
      "Epoch [2/20], Step [29/88], Loss: 0.4907\n",
      "Epoch [2/20], Step [30/88], Loss: 0.4301\n",
      "Epoch [2/20], Step [31/88], Loss: 0.5567\n",
      "Epoch [2/20], Step [32/88], Loss: 0.4188\n",
      "Epoch [2/20], Step [33/88], Loss: 0.5579\n",
      "Epoch [2/20], Step [34/88], Loss: 0.5705\n",
      "Epoch [2/20], Step [35/88], Loss: 0.3460\n",
      "Epoch [2/20], Step [36/88], Loss: 0.4917\n",
      "Epoch [2/20], Step [37/88], Loss: 0.3224\n",
      "Epoch [2/20], Step [38/88], Loss: 0.5181\n",
      "Epoch [2/20], Step [39/88], Loss: 0.4569\n",
      "Epoch [2/20], Step [40/88], Loss: 0.4579\n",
      "Epoch [2/20], Step [41/88], Loss: 0.3973\n",
      "Epoch [2/20], Step [42/88], Loss: 0.4410\n",
      "Epoch [2/20], Step [43/88], Loss: 0.4332\n",
      "Epoch [2/20], Step [44/88], Loss: 0.4425\n",
      "Epoch [2/20], Step [45/88], Loss: 0.6497\n",
      "Epoch [2/20], Step [46/88], Loss: 0.2712\n",
      "Epoch [2/20], Step [47/88], Loss: 0.5024\n",
      "Epoch [2/20], Step [48/88], Loss: 0.5198\n",
      "Epoch [2/20], Step [49/88], Loss: 0.4444\n",
      "Epoch [2/20], Step [50/88], Loss: 0.4000\n",
      "Epoch [2/20], Step [51/88], Loss: 0.6443\n",
      "Epoch [2/20], Step [52/88], Loss: 0.3060\n",
      "Epoch [2/20], Step [53/88], Loss: 0.5719\n",
      "Epoch [2/20], Step [54/88], Loss: 0.4058\n",
      "Epoch [2/20], Step [55/88], Loss: 0.3620\n",
      "Epoch [2/20], Step [56/88], Loss: 0.4369\n",
      "Epoch [2/20], Step [57/88], Loss: 0.3261\n",
      "Epoch [2/20], Step [58/88], Loss: 0.4499\n",
      "Epoch [2/20], Step [59/88], Loss: 0.4148\n",
      "Epoch [2/20], Step [60/88], Loss: 0.3538\n",
      "Epoch [2/20], Step [61/88], Loss: 0.5285\n",
      "Epoch [2/20], Step [62/88], Loss: 0.5492\n",
      "Epoch [2/20], Step [63/88], Loss: 0.4140\n",
      "Epoch [2/20], Step [64/88], Loss: 0.4923\n",
      "Epoch [2/20], Step [65/88], Loss: 0.4712\n",
      "Epoch [2/20], Step [66/88], Loss: 0.3344\n",
      "Epoch [2/20], Step [67/88], Loss: 0.3852\n",
      "Epoch [2/20], Step [68/88], Loss: 0.4293\n",
      "Epoch [2/20], Step [69/88], Loss: 0.3654\n",
      "Epoch [2/20], Step [70/88], Loss: 0.3744\n",
      "Epoch [2/20], Step [71/88], Loss: 0.5751\n",
      "Epoch [2/20], Step [72/88], Loss: 0.5426\n",
      "Epoch [2/20], Step [73/88], Loss: 0.3719\n",
      "Epoch [2/20], Step [74/88], Loss: 0.4244\n",
      "Epoch [2/20], Step [75/88], Loss: 0.4562\n",
      "Epoch [2/20], Step [76/88], Loss: 0.5115\n",
      "Epoch [2/20], Step [77/88], Loss: 0.5782\n",
      "Epoch [2/20], Step [78/88], Loss: 0.3325\n",
      "Epoch [2/20], Step [79/88], Loss: 0.3887\n",
      "Epoch [2/20], Step [80/88], Loss: 0.4256\n",
      "Epoch [2/20], Step [81/88], Loss: 0.3107\n",
      "Epoch [2/20], Step [82/88], Loss: 0.3914\n",
      "Epoch [2/20], Step [83/88], Loss: 0.5183\n",
      "Epoch [2/20], Step [84/88], Loss: 0.4817\n",
      "Epoch [2/20], Step [85/88], Loss: 0.2716\n",
      "Epoch [2/20], Step [86/88], Loss: 0.6753\n",
      "Epoch [2/20], Step [87/88], Loss: 0.4631\n",
      "Epoch [2/20], Step [88/88], Loss: 0.3946\n",
      "Accuracy at 2 epoch: 0.77%\n",
      "Epoch [3/20], Step [1/88], Loss: 0.3049\n",
      "Epoch [3/20], Step [2/88], Loss: 0.4291\n",
      "Epoch [3/20], Step [3/88], Loss: 0.3354\n",
      "Epoch [3/20], Step [4/88], Loss: 0.2984\n",
      "Epoch [3/20], Step [5/88], Loss: 0.4116\n",
      "Epoch [3/20], Step [6/88], Loss: 0.4114\n",
      "Epoch [3/20], Step [7/88], Loss: 0.4422\n",
      "Epoch [3/20], Step [8/88], Loss: 0.3871\n",
      "Epoch [3/20], Step [9/88], Loss: 0.5663\n",
      "Epoch [3/20], Step [10/88], Loss: 0.4509\n",
      "Epoch [3/20], Step [11/88], Loss: 0.3780\n",
      "Epoch [3/20], Step [12/88], Loss: 0.2864\n",
      "Epoch [3/20], Step [13/88], Loss: 0.5145\n",
      "Epoch [3/20], Step [14/88], Loss: 0.4204\n",
      "Epoch [3/20], Step [15/88], Loss: 0.2833\n",
      "Epoch [3/20], Step [16/88], Loss: 0.3128\n",
      "Epoch [3/20], Step [17/88], Loss: 0.2872\n",
      "Epoch [3/20], Step [18/88], Loss: 0.2560\n",
      "Epoch [3/20], Step [19/88], Loss: 0.4324\n",
      "Epoch [3/20], Step [20/88], Loss: 0.6426\n",
      "Epoch [3/20], Step [21/88], Loss: 0.3633\n",
      "Epoch [3/20], Step [22/88], Loss: 0.3717\n",
      "Epoch [3/20], Step [23/88], Loss: 0.5318\n",
      "Epoch [3/20], Step [24/88], Loss: 0.5727\n",
      "Epoch [3/20], Step [25/88], Loss: 0.7673\n",
      "Epoch [3/20], Step [26/88], Loss: 0.2789\n",
      "Epoch [3/20], Step [27/88], Loss: 0.4230\n",
      "Epoch [3/20], Step [28/88], Loss: 0.4239\n",
      "Epoch [3/20], Step [29/88], Loss: 0.3081\n",
      "Epoch [3/20], Step [30/88], Loss: 0.3309\n",
      "Epoch [3/20], Step [31/88], Loss: 0.4084\n",
      "Epoch [3/20], Step [32/88], Loss: 0.2311\n",
      "Epoch [3/20], Step [33/88], Loss: 0.3748\n",
      "Epoch [3/20], Step [34/88], Loss: 0.2603\n",
      "Epoch [3/20], Step [35/88], Loss: 0.4176\n",
      "Epoch [3/20], Step [36/88], Loss: 0.3284\n",
      "Epoch [3/20], Step [37/88], Loss: 0.5081\n",
      "Epoch [3/20], Step [38/88], Loss: 0.4465\n",
      "Epoch [3/20], Step [39/88], Loss: 0.2470\n",
      "Epoch [3/20], Step [40/88], Loss: 0.2968\n",
      "Epoch [3/20], Step [41/88], Loss: 0.2850\n",
      "Epoch [3/20], Step [42/88], Loss: 0.3515\n",
      "Epoch [3/20], Step [43/88], Loss: 0.5635\n",
      "Epoch [3/20], Step [44/88], Loss: 0.5064\n",
      "Epoch [3/20], Step [45/88], Loss: 0.3172\n",
      "Epoch [3/20], Step [46/88], Loss: 0.4044\n",
      "Epoch [3/20], Step [47/88], Loss: 0.2496\n",
      "Epoch [3/20], Step [48/88], Loss: 0.5250\n",
      "Epoch [3/20], Step [49/88], Loss: 0.3741\n",
      "Epoch [3/20], Step [50/88], Loss: 0.3893\n",
      "Epoch [3/20], Step [51/88], Loss: 0.2849\n",
      "Epoch [3/20], Step [52/88], Loss: 0.3289\n",
      "Epoch [3/20], Step [53/88], Loss: 0.2604\n",
      "Epoch [3/20], Step [54/88], Loss: 0.3385\n",
      "Epoch [3/20], Step [55/88], Loss: 0.2275\n",
      "Epoch [3/20], Step [56/88], Loss: 0.2394\n",
      "Epoch [3/20], Step [57/88], Loss: 0.3291\n",
      "Epoch [3/20], Step [58/88], Loss: 0.3512\n",
      "Epoch [3/20], Step [59/88], Loss: 0.2913\n",
      "Epoch [3/20], Step [60/88], Loss: 0.2827\n",
      "Epoch [3/20], Step [61/88], Loss: 0.4679\n",
      "Epoch [3/20], Step [62/88], Loss: 0.2603\n",
      "Epoch [3/20], Step [63/88], Loss: 0.1985\n",
      "Epoch [3/20], Step [64/88], Loss: 0.2969\n",
      "Epoch [3/20], Step [65/88], Loss: 0.2412\n",
      "Epoch [3/20], Step [66/88], Loss: 0.3351\n",
      "Epoch [3/20], Step [67/88], Loss: 0.1667\n",
      "Epoch [3/20], Step [68/88], Loss: 0.8273\n",
      "Epoch [3/20], Step [69/88], Loss: 0.2981\n",
      "Epoch [3/20], Step [70/88], Loss: 0.4930\n",
      "Epoch [3/20], Step [71/88], Loss: 0.2639\n",
      "Epoch [3/20], Step [72/88], Loss: 0.4087\n",
      "Epoch [3/20], Step [73/88], Loss: 0.5252\n",
      "Epoch [3/20], Step [74/88], Loss: 0.4100\n",
      "Epoch [3/20], Step [75/88], Loss: 0.5067\n",
      "Epoch [3/20], Step [76/88], Loss: 0.3174\n",
      "Epoch [3/20], Step [77/88], Loss: 0.4901\n",
      "Epoch [3/20], Step [78/88], Loss: 0.5234\n",
      "Epoch [3/20], Step [79/88], Loss: 0.4136\n",
      "Epoch [3/20], Step [80/88], Loss: 0.3262\n",
      "Epoch [3/20], Step [81/88], Loss: 0.2853\n",
      "Epoch [3/20], Step [82/88], Loss: 0.2498\n",
      "Epoch [3/20], Step [83/88], Loss: 0.4128\n",
      "Epoch [3/20], Step [84/88], Loss: 0.4194\n",
      "Epoch [3/20], Step [85/88], Loss: 0.4585\n",
      "Epoch [3/20], Step [86/88], Loss: 0.3784\n",
      "Epoch [3/20], Step [87/88], Loss: 0.5853\n",
      "Epoch [3/20], Step [88/88], Loss: 0.1124\n",
      "Accuracy at 3 epoch: 0.82%\n",
      "Epoch [4/20], Step [1/88], Loss: 0.3618\n",
      "Epoch [4/20], Step [2/88], Loss: 0.1566\n",
      "Epoch [4/20], Step [3/88], Loss: 0.2474\n",
      "Epoch [4/20], Step [4/88], Loss: 0.4994\n",
      "Epoch [4/20], Step [5/88], Loss: 0.3636\n",
      "Epoch [4/20], Step [6/88], Loss: 0.2417\n",
      "Epoch [4/20], Step [7/88], Loss: 0.3141\n",
      "Epoch [4/20], Step [8/88], Loss: 0.3637\n",
      "Epoch [4/20], Step [9/88], Loss: 0.4580\n",
      "Epoch [4/20], Step [10/88], Loss: 0.3428\n",
      "Epoch [4/20], Step [11/88], Loss: 0.2485\n",
      "Epoch [4/20], Step [12/88], Loss: 0.4217\n",
      "Epoch [4/20], Step [13/88], Loss: 0.4304\n",
      "Epoch [4/20], Step [14/88], Loss: 0.3800\n",
      "Epoch [4/20], Step [15/88], Loss: 0.3626\n",
      "Epoch [4/20], Step [16/88], Loss: 0.3534\n",
      "Epoch [4/20], Step [17/88], Loss: 0.5603\n",
      "Epoch [4/20], Step [18/88], Loss: 0.3091\n",
      "Epoch [4/20], Step [19/88], Loss: 0.4603\n",
      "Epoch [4/20], Step [20/88], Loss: 0.3590\n",
      "Epoch [4/20], Step [21/88], Loss: 0.3140\n",
      "Epoch [4/20], Step [22/88], Loss: 0.1881\n",
      "Epoch [4/20], Step [23/88], Loss: 0.3485\n",
      "Epoch [4/20], Step [24/88], Loss: 0.2428\n",
      "Epoch [4/20], Step [25/88], Loss: 0.4003\n",
      "Epoch [4/20], Step [26/88], Loss: 0.3807\n",
      "Epoch [4/20], Step [27/88], Loss: 0.6440\n",
      "Epoch [4/20], Step [28/88], Loss: 0.2843\n",
      "Epoch [4/20], Step [29/88], Loss: 0.3168\n",
      "Epoch [4/20], Step [30/88], Loss: 0.3170\n",
      "Epoch [4/20], Step [31/88], Loss: 0.2808\n",
      "Epoch [4/20], Step [32/88], Loss: 0.1257\n",
      "Epoch [4/20], Step [33/88], Loss: 0.4611\n",
      "Epoch [4/20], Step [34/88], Loss: 0.3583\n",
      "Epoch [4/20], Step [35/88], Loss: 0.3059\n",
      "Epoch [4/20], Step [36/88], Loss: 0.3130\n",
      "Epoch [4/20], Step [37/88], Loss: 0.2584\n",
      "Epoch [4/20], Step [38/88], Loss: 0.4332\n",
      "Epoch [4/20], Step [39/88], Loss: 0.3070\n",
      "Epoch [4/20], Step [40/88], Loss: 0.2255\n",
      "Epoch [4/20], Step [41/88], Loss: 0.2780\n",
      "Epoch [4/20], Step [42/88], Loss: 0.2193\n",
      "Epoch [4/20], Step [43/88], Loss: 0.3918\n",
      "Epoch [4/20], Step [44/88], Loss: 0.2426\n",
      "Epoch [4/20], Step [45/88], Loss: 0.3387\n",
      "Epoch [4/20], Step [46/88], Loss: 0.3644\n",
      "Epoch [4/20], Step [47/88], Loss: 0.1632\n",
      "Epoch [4/20], Step [48/88], Loss: 0.2373\n",
      "Epoch [4/20], Step [49/88], Loss: 0.3112\n",
      "Epoch [4/20], Step [50/88], Loss: 0.2461\n",
      "Epoch [4/20], Step [51/88], Loss: 0.4289\n",
      "Epoch [4/20], Step [52/88], Loss: 0.2674\n",
      "Epoch [4/20], Step [53/88], Loss: 0.2842\n",
      "Epoch [4/20], Step [54/88], Loss: 0.4065\n",
      "Epoch [4/20], Step [55/88], Loss: 0.3454\n",
      "Epoch [4/20], Step [56/88], Loss: 0.1686\n",
      "Epoch [4/20], Step [57/88], Loss: 0.1792\n",
      "Epoch [4/20], Step [58/88], Loss: 0.1829\n",
      "Epoch [4/20], Step [59/88], Loss: 0.4266\n",
      "Epoch [4/20], Step [60/88], Loss: 0.2333\n",
      "Epoch [4/20], Step [61/88], Loss: 0.3030\n",
      "Epoch [4/20], Step [62/88], Loss: 0.3180\n",
      "Epoch [4/20], Step [63/88], Loss: 0.2732\n",
      "Epoch [4/20], Step [64/88], Loss: 0.2429\n",
      "Epoch [4/20], Step [65/88], Loss: 0.4809\n",
      "Epoch [4/20], Step [66/88], Loss: 0.2581\n",
      "Epoch [4/20], Step [67/88], Loss: 0.2289\n",
      "Epoch [4/20], Step [68/88], Loss: 0.3827\n",
      "Epoch [4/20], Step [69/88], Loss: 0.1385\n",
      "Epoch [4/20], Step [70/88], Loss: 0.1733\n",
      "Epoch [4/20], Step [71/88], Loss: 0.1965\n",
      "Epoch [4/20], Step [72/88], Loss: 0.3283\n",
      "Epoch [4/20], Step [73/88], Loss: 0.5934\n",
      "Epoch [4/20], Step [74/88], Loss: 0.2653\n",
      "Epoch [4/20], Step [75/88], Loss: 0.4058\n",
      "Epoch [4/20], Step [76/88], Loss: 0.2766\n",
      "Epoch [4/20], Step [77/88], Loss: 0.3638\n",
      "Epoch [4/20], Step [78/88], Loss: 0.3784\n",
      "Epoch [4/20], Step [79/88], Loss: 0.3092\n",
      "Epoch [4/20], Step [80/88], Loss: 0.2580\n",
      "Epoch [4/20], Step [81/88], Loss: 0.3682\n",
      "Epoch [4/20], Step [82/88], Loss: 0.2490\n",
      "Epoch [4/20], Step [83/88], Loss: 0.3214\n",
      "Epoch [4/20], Step [84/88], Loss: 0.3750\n",
      "Epoch [4/20], Step [85/88], Loss: 0.1025\n",
      "Epoch [4/20], Step [86/88], Loss: 0.1898\n",
      "Epoch [4/20], Step [87/88], Loss: 0.3081\n",
      "Epoch [4/20], Step [88/88], Loss: 0.2549\n",
      "Accuracy at 4 epoch: 0.85%\n",
      "Epoch [5/20], Step [1/88], Loss: 0.3316\n",
      "Epoch [5/20], Step [2/88], Loss: 0.2275\n",
      "Epoch [5/20], Step [3/88], Loss: 0.1443\n",
      "Epoch [5/20], Step [4/88], Loss: 0.2892\n",
      "Epoch [5/20], Step [5/88], Loss: 0.2330\n",
      "Epoch [5/20], Step [6/88], Loss: 0.0839\n",
      "Epoch [5/20], Step [7/88], Loss: 0.1505\n",
      "Epoch [5/20], Step [8/88], Loss: 0.2160\n",
      "Epoch [5/20], Step [9/88], Loss: 0.2041\n",
      "Epoch [5/20], Step [10/88], Loss: 0.6126\n",
      "Epoch [5/20], Step [11/88], Loss: 0.2040\n",
      "Epoch [5/20], Step [12/88], Loss: 0.1724\n",
      "Epoch [5/20], Step [13/88], Loss: 0.2219\n",
      "Epoch [5/20], Step [14/88], Loss: 0.2915\n",
      "Epoch [5/20], Step [15/88], Loss: 0.1123\n",
      "Epoch [5/20], Step [16/88], Loss: 0.1609\n",
      "Epoch [5/20], Step [17/88], Loss: 0.2703\n",
      "Epoch [5/20], Step [18/88], Loss: 0.3016\n",
      "Epoch [5/20], Step [19/88], Loss: 0.3111\n",
      "Epoch [5/20], Step [20/88], Loss: 0.3119\n",
      "Epoch [5/20], Step [21/88], Loss: 0.2991\n",
      "Epoch [5/20], Step [22/88], Loss: 0.2513\n",
      "Epoch [5/20], Step [23/88], Loss: 0.3463\n",
      "Epoch [5/20], Step [24/88], Loss: 0.3309\n",
      "Epoch [5/20], Step [25/88], Loss: 0.1929\n",
      "Epoch [5/20], Step [26/88], Loss: 0.2098\n",
      "Epoch [5/20], Step [27/88], Loss: 0.2763\n",
      "Epoch [5/20], Step [28/88], Loss: 0.3521\n",
      "Epoch [5/20], Step [29/88], Loss: 0.3397\n",
      "Epoch [5/20], Step [30/88], Loss: 0.3093\n",
      "Epoch [5/20], Step [31/88], Loss: 0.1964\n",
      "Epoch [5/20], Step [32/88], Loss: 0.2858\n",
      "Epoch [5/20], Step [33/88], Loss: 0.1038\n",
      "Epoch [5/20], Step [34/88], Loss: 0.1408\n",
      "Epoch [5/20], Step [35/88], Loss: 0.3380\n",
      "Epoch [5/20], Step [36/88], Loss: 0.3084\n",
      "Epoch [5/20], Step [37/88], Loss: 0.1658\n",
      "Epoch [5/20], Step [38/88], Loss: 0.3183\n",
      "Epoch [5/20], Step [39/88], Loss: 0.2951\n",
      "Epoch [5/20], Step [40/88], Loss: 0.1781\n",
      "Epoch [5/20], Step [41/88], Loss: 0.8173\n",
      "Epoch [5/20], Step [42/88], Loss: 0.3895\n",
      "Epoch [5/20], Step [43/88], Loss: 0.1496\n",
      "Epoch [5/20], Step [44/88], Loss: 0.2355\n",
      "Epoch [5/20], Step [45/88], Loss: 0.1483\n",
      "Epoch [5/20], Step [46/88], Loss: 0.2376\n",
      "Epoch [5/20], Step [47/88], Loss: 0.3193\n",
      "Epoch [5/20], Step [48/88], Loss: 0.2713\n",
      "Epoch [5/20], Step [49/88], Loss: 0.5920\n",
      "Epoch [5/20], Step [50/88], Loss: 0.4536\n",
      "Epoch [5/20], Step [51/88], Loss: 0.3300\n",
      "Epoch [5/20], Step [52/88], Loss: 0.2293\n",
      "Epoch [5/20], Step [53/88], Loss: 0.1635\n",
      "Epoch [5/20], Step [54/88], Loss: 0.2991\n",
      "Epoch [5/20], Step [55/88], Loss: 0.3871\n",
      "Epoch [5/20], Step [56/88], Loss: 0.2522\n",
      "Epoch [5/20], Step [57/88], Loss: 0.1937\n",
      "Epoch [5/20], Step [58/88], Loss: 0.2855\n",
      "Epoch [5/20], Step [59/88], Loss: 0.3186\n",
      "Epoch [5/20], Step [60/88], Loss: 0.2921\n",
      "Epoch [5/20], Step [61/88], Loss: 0.4846\n",
      "Epoch [5/20], Step [62/88], Loss: 0.3203\n",
      "Epoch [5/20], Step [63/88], Loss: 0.2481\n",
      "Epoch [5/20], Step [64/88], Loss: 0.2263\n",
      "Epoch [5/20], Step [65/88], Loss: 0.2692\n",
      "Epoch [5/20], Step [66/88], Loss: 0.2778\n",
      "Epoch [5/20], Step [67/88], Loss: 0.3948\n",
      "Epoch [5/20], Step [68/88], Loss: 0.1680\n",
      "Epoch [5/20], Step [69/88], Loss: 0.2542\n",
      "Epoch [5/20], Step [70/88], Loss: 0.3968\n",
      "Epoch [5/20], Step [71/88], Loss: 0.2624\n",
      "Epoch [5/20], Step [72/88], Loss: 0.2402\n",
      "Epoch [5/20], Step [73/88], Loss: 0.2372\n",
      "Epoch [5/20], Step [74/88], Loss: 0.3658\n",
      "Epoch [5/20], Step [75/88], Loss: 0.2688\n",
      "Epoch [5/20], Step [76/88], Loss: 0.2592\n",
      "Epoch [5/20], Step [77/88], Loss: 0.2100\n",
      "Epoch [5/20], Step [78/88], Loss: 0.3956\n",
      "Epoch [5/20], Step [79/88], Loss: 0.2437\n",
      "Epoch [5/20], Step [80/88], Loss: 0.1816\n",
      "Epoch [5/20], Step [81/88], Loss: 0.0969\n",
      "Epoch [5/20], Step [82/88], Loss: 0.1526\n",
      "Epoch [5/20], Step [83/88], Loss: 0.3143\n",
      "Epoch [5/20], Step [84/88], Loss: 0.1566\n",
      "Epoch [5/20], Step [85/88], Loss: 0.2522\n",
      "Epoch [5/20], Step [86/88], Loss: 0.1131\n",
      "Epoch [5/20], Step [87/88], Loss: 0.3651\n",
      "Epoch [5/20], Step [88/88], Loss: 0.7829\n",
      "Accuracy at 5 epoch: 0.89%\n",
      "Epoch [6/20], Step [1/88], Loss: 0.4267\n",
      "Epoch [6/20], Step [2/88], Loss: 0.1950\n",
      "Epoch [6/20], Step [3/88], Loss: 0.3224\n",
      "Epoch [6/20], Step [4/88], Loss: 0.1933\n",
      "Epoch [6/20], Step [5/88], Loss: 0.3034\n",
      "Epoch [6/20], Step [6/88], Loss: 0.2233\n",
      "Epoch [6/20], Step [7/88], Loss: 0.0754\n",
      "Epoch [6/20], Step [8/88], Loss: 0.3545\n",
      "Epoch [6/20], Step [9/88], Loss: 0.1548\n",
      "Epoch [6/20], Step [10/88], Loss: 0.1585\n",
      "Epoch [6/20], Step [11/88], Loss: 0.3634\n",
      "Epoch [6/20], Step [12/88], Loss: 0.3412\n",
      "Epoch [6/20], Step [13/88], Loss: 0.1833\n",
      "Epoch [6/20], Step [14/88], Loss: 0.2222\n",
      "Epoch [6/20], Step [15/88], Loss: 0.1407\n",
      "Epoch [6/20], Step [16/88], Loss: 0.1829\n",
      "Epoch [6/20], Step [17/88], Loss: 0.2335\n",
      "Epoch [6/20], Step [18/88], Loss: 0.1307\n",
      "Epoch [6/20], Step [19/88], Loss: 0.1901\n",
      "Epoch [6/20], Step [20/88], Loss: 0.1693\n",
      "Epoch [6/20], Step [21/88], Loss: 0.2978\n",
      "Epoch [6/20], Step [22/88], Loss: 0.1426\n",
      "Epoch [6/20], Step [23/88], Loss: 0.2992\n",
      "Epoch [6/20], Step [24/88], Loss: 0.2740\n",
      "Epoch [6/20], Step [25/88], Loss: 0.1298\n",
      "Epoch [6/20], Step [26/88], Loss: 0.2665\n",
      "Epoch [6/20], Step [27/88], Loss: 0.2789\n",
      "Epoch [6/20], Step [28/88], Loss: 0.1212\n",
      "Epoch [6/20], Step [29/88], Loss: 0.2641\n",
      "Epoch [6/20], Step [30/88], Loss: 0.1966\n",
      "Epoch [6/20], Step [31/88], Loss: 0.1304\n",
      "Epoch [6/20], Step [32/88], Loss: 0.1572\n",
      "Epoch [6/20], Step [33/88], Loss: 0.4860\n",
      "Epoch [6/20], Step [34/88], Loss: 0.1266\n",
      "Epoch [6/20], Step [35/88], Loss: 0.2109\n",
      "Epoch [6/20], Step [36/88], Loss: 0.2400\n",
      "Epoch [6/20], Step [37/88], Loss: 0.3049\n",
      "Epoch [6/20], Step [38/88], Loss: 0.0953\n",
      "Epoch [6/20], Step [39/88], Loss: 0.1726\n",
      "Epoch [6/20], Step [40/88], Loss: 0.1372\n",
      "Epoch [6/20], Step [41/88], Loss: 0.3454\n",
      "Epoch [6/20], Step [42/88], Loss: 0.0863\n",
      "Epoch [6/20], Step [43/88], Loss: 0.1326\n",
      "Epoch [6/20], Step [44/88], Loss: 0.1700\n",
      "Epoch [6/20], Step [45/88], Loss: 0.2591\n",
      "Epoch [6/20], Step [46/88], Loss: 0.2391\n",
      "Epoch [6/20], Step [47/88], Loss: 0.2620\n",
      "Epoch [6/20], Step [48/88], Loss: 0.1976\n",
      "Epoch [6/20], Step [49/88], Loss: 0.2362\n",
      "Epoch [6/20], Step [50/88], Loss: 0.1484\n",
      "Epoch [6/20], Step [51/88], Loss: 0.1655\n",
      "Epoch [6/20], Step [52/88], Loss: 0.4399\n",
      "Epoch [6/20], Step [53/88], Loss: 0.3648\n",
      "Epoch [6/20], Step [54/88], Loss: 0.1467\n",
      "Epoch [6/20], Step [55/88], Loss: 0.1631\n",
      "Epoch [6/20], Step [56/88], Loss: 0.1719\n",
      "Epoch [6/20], Step [57/88], Loss: 0.3235\n",
      "Epoch [6/20], Step [58/88], Loss: 0.5610\n",
      "Epoch [6/20], Step [59/88], Loss: 0.0701\n",
      "Epoch [6/20], Step [60/88], Loss: 0.2998\n",
      "Epoch [6/20], Step [61/88], Loss: 0.2525\n",
      "Epoch [6/20], Step [62/88], Loss: 0.2212\n",
      "Epoch [6/20], Step [63/88], Loss: 0.2252\n",
      "Epoch [6/20], Step [64/88], Loss: 0.4413\n",
      "Epoch [6/20], Step [65/88], Loss: 0.1807\n",
      "Epoch [6/20], Step [66/88], Loss: 0.2227\n",
      "Epoch [6/20], Step [67/88], Loss: 0.0886\n",
      "Epoch [6/20], Step [68/88], Loss: 0.5968\n",
      "Epoch [6/20], Step [69/88], Loss: 0.3961\n",
      "Epoch [6/20], Step [70/88], Loss: 0.1805\n",
      "Epoch [6/20], Step [71/88], Loss: 0.2200\n",
      "Epoch [6/20], Step [72/88], Loss: 0.2083\n",
      "Epoch [6/20], Step [73/88], Loss: 0.2211\n",
      "Epoch [6/20], Step [74/88], Loss: 0.3822\n",
      "Epoch [6/20], Step [75/88], Loss: 0.1503\n",
      "Epoch [6/20], Step [76/88], Loss: 0.1588\n",
      "Epoch [6/20], Step [77/88], Loss: 0.1543\n",
      "Epoch [6/20], Step [78/88], Loss: 0.2880\n",
      "Epoch [6/20], Step [79/88], Loss: 0.2376\n",
      "Epoch [6/20], Step [80/88], Loss: 0.4606\n",
      "Epoch [6/20], Step [81/88], Loss: 0.3708\n",
      "Epoch [6/20], Step [82/88], Loss: 0.2645\n",
      "Epoch [6/20], Step [83/88], Loss: 0.2744\n",
      "Epoch [6/20], Step [84/88], Loss: 0.1547\n",
      "Epoch [6/20], Step [85/88], Loss: 0.3806\n",
      "Epoch [6/20], Step [86/88], Loss: 0.1262\n",
      "Epoch [6/20], Step [87/88], Loss: 0.3824\n",
      "Epoch [6/20], Step [88/88], Loss: 0.1633\n",
      "Accuracy at 6 epoch: 0.90%\n",
      "Epoch [7/20], Step [1/88], Loss: 0.2202\n",
      "Epoch [7/20], Step [2/88], Loss: 0.1278\n",
      "Epoch [7/20], Step [3/88], Loss: 0.2222\n",
      "Epoch [7/20], Step [4/88], Loss: 0.2472\n",
      "Epoch [7/20], Step [5/88], Loss: 0.1699\n",
      "Epoch [7/20], Step [6/88], Loss: 0.2985\n",
      "Epoch [7/20], Step [7/88], Loss: 0.3228\n",
      "Epoch [7/20], Step [8/88], Loss: 0.3792\n",
      "Epoch [7/20], Step [9/88], Loss: 0.3288\n",
      "Epoch [7/20], Step [10/88], Loss: 0.1525\n",
      "Epoch [7/20], Step [11/88], Loss: 0.2208\n",
      "Epoch [7/20], Step [12/88], Loss: 0.1891\n",
      "Epoch [7/20], Step [13/88], Loss: 0.2626\n",
      "Epoch [7/20], Step [14/88], Loss: 0.1344\n",
      "Epoch [7/20], Step [15/88], Loss: 0.1902\n",
      "Epoch [7/20], Step [16/88], Loss: 0.1085\n",
      "Epoch [7/20], Step [17/88], Loss: 0.1631\n",
      "Epoch [7/20], Step [18/88], Loss: 0.1787\n",
      "Epoch [7/20], Step [19/88], Loss: 0.2022\n",
      "Epoch [7/20], Step [20/88], Loss: 0.1422\n",
      "Epoch [7/20], Step [21/88], Loss: 0.2594\n",
      "Epoch [7/20], Step [22/88], Loss: 0.2460\n",
      "Epoch [7/20], Step [23/88], Loss: 0.2518\n",
      "Epoch [7/20], Step [24/88], Loss: 0.1912\n",
      "Epoch [7/20], Step [25/88], Loss: 0.1626\n",
      "Epoch [7/20], Step [26/88], Loss: 0.2191\n",
      "Epoch [7/20], Step [27/88], Loss: 0.1444\n",
      "Epoch [7/20], Step [28/88], Loss: 0.3638\n",
      "Epoch [7/20], Step [29/88], Loss: 0.1143\n",
      "Epoch [7/20], Step [30/88], Loss: 0.3045\n",
      "Epoch [7/20], Step [31/88], Loss: 0.1747\n",
      "Epoch [7/20], Step [32/88], Loss: 0.1512\n",
      "Epoch [7/20], Step [33/88], Loss: 0.0778\n",
      "Epoch [7/20], Step [34/88], Loss: 0.1233\n",
      "Epoch [7/20], Step [35/88], Loss: 0.3380\n",
      "Epoch [7/20], Step [36/88], Loss: 0.0980\n",
      "Epoch [7/20], Step [37/88], Loss: 0.1286\n",
      "Epoch [7/20], Step [38/88], Loss: 0.4337\n",
      "Epoch [7/20], Step [39/88], Loss: 0.1271\n",
      "Epoch [7/20], Step [40/88], Loss: 0.0808\n",
      "Epoch [7/20], Step [41/88], Loss: 0.1610\n",
      "Epoch [7/20], Step [42/88], Loss: 0.2750\n",
      "Epoch [7/20], Step [43/88], Loss: 0.4035\n",
      "Epoch [7/20], Step [44/88], Loss: 0.2040\n",
      "Epoch [7/20], Step [45/88], Loss: 0.2349\n",
      "Epoch [7/20], Step [46/88], Loss: 0.2413\n",
      "Epoch [7/20], Step [47/88], Loss: 0.1861\n",
      "Epoch [7/20], Step [48/88], Loss: 0.1105\n",
      "Epoch [7/20], Step [49/88], Loss: 0.3271\n",
      "Epoch [7/20], Step [50/88], Loss: 0.1729\n",
      "Epoch [7/20], Step [51/88], Loss: 0.1857\n",
      "Epoch [7/20], Step [52/88], Loss: 0.2171\n",
      "Epoch [7/20], Step [53/88], Loss: 0.1590\n",
      "Epoch [7/20], Step [54/88], Loss: 0.2488\n",
      "Epoch [7/20], Step [55/88], Loss: 0.2123\n",
      "Epoch [7/20], Step [56/88], Loss: 0.2983\n",
      "Epoch [7/20], Step [57/88], Loss: 0.0745\n",
      "Epoch [7/20], Step [58/88], Loss: 0.1196\n",
      "Epoch [7/20], Step [59/88], Loss: 0.3098\n",
      "Epoch [7/20], Step [60/88], Loss: 0.1158\n",
      "Epoch [7/20], Step [61/88], Loss: 0.4221\n",
      "Epoch [7/20], Step [62/88], Loss: 0.1044\n",
      "Epoch [7/20], Step [63/88], Loss: 0.2228\n",
      "Epoch [7/20], Step [64/88], Loss: 0.1735\n",
      "Epoch [7/20], Step [65/88], Loss: 0.3246\n",
      "Epoch [7/20], Step [66/88], Loss: 0.3926\n",
      "Epoch [7/20], Step [67/88], Loss: 0.2021\n",
      "Epoch [7/20], Step [68/88], Loss: 0.1466\n",
      "Epoch [7/20], Step [69/88], Loss: 0.3156\n",
      "Epoch [7/20], Step [70/88], Loss: 0.0829\n",
      "Epoch [7/20], Step [71/88], Loss: 0.1308\n",
      "Epoch [7/20], Step [72/88], Loss: 0.2520\n",
      "Epoch [7/20], Step [73/88], Loss: 0.2108\n",
      "Epoch [7/20], Step [74/88], Loss: 0.1868\n",
      "Epoch [7/20], Step [75/88], Loss: 0.1683\n",
      "Epoch [7/20], Step [76/88], Loss: 0.1352\n",
      "Epoch [7/20], Step [77/88], Loss: 0.2365\n",
      "Epoch [7/20], Step [78/88], Loss: 0.2123\n",
      "Epoch [7/20], Step [79/88], Loss: 0.0724\n",
      "Epoch [7/20], Step [80/88], Loss: 0.1332\n",
      "Epoch [7/20], Step [81/88], Loss: 0.0832\n",
      "Epoch [7/20], Step [82/88], Loss: 0.0829\n",
      "Epoch [7/20], Step [83/88], Loss: 0.2023\n",
      "Epoch [7/20], Step [84/88], Loss: 0.2618\n",
      "Epoch [7/20], Step [85/88], Loss: 0.1752\n",
      "Epoch [7/20], Step [86/88], Loss: 0.2272\n",
      "Epoch [7/20], Step [87/88], Loss: 0.2436\n",
      "Epoch [7/20], Step [88/88], Loss: 0.2571\n",
      "Accuracy at 7 epoch: 0.92%\n",
      "Epoch [8/20], Step [1/88], Loss: 0.3328\n",
      "Epoch [8/20], Step [2/88], Loss: 0.1512\n",
      "Epoch [8/20], Step [3/88], Loss: 0.2077\n",
      "Epoch [8/20], Step [4/88], Loss: 0.1038\n",
      "Epoch [8/20], Step [5/88], Loss: 0.0807\n",
      "Epoch [8/20], Step [6/88], Loss: 0.2131\n",
      "Epoch [8/20], Step [7/88], Loss: 0.3522\n",
      "Epoch [8/20], Step [8/88], Loss: 0.1478\n",
      "Epoch [8/20], Step [9/88], Loss: 0.3868\n",
      "Epoch [8/20], Step [10/88], Loss: 0.2656\n",
      "Epoch [8/20], Step [11/88], Loss: 0.2137\n",
      "Epoch [8/20], Step [12/88], Loss: 0.1772\n",
      "Epoch [8/20], Step [13/88], Loss: 0.1006\n",
      "Epoch [8/20], Step [14/88], Loss: 0.1317\n",
      "Epoch [8/20], Step [15/88], Loss: 0.0252\n",
      "Epoch [8/20], Step [16/88], Loss: 0.2077\n",
      "Epoch [8/20], Step [17/88], Loss: 0.0720\n",
      "Epoch [8/20], Step [18/88], Loss: 0.3447\n",
      "Epoch [8/20], Step [19/88], Loss: 0.1570\n",
      "Epoch [8/20], Step [20/88], Loss: 0.2593\n",
      "Epoch [8/20], Step [21/88], Loss: 0.0923\n",
      "Epoch [8/20], Step [22/88], Loss: 0.1302\n",
      "Epoch [8/20], Step [23/88], Loss: 0.0980\n",
      "Epoch [8/20], Step [24/88], Loss: 0.1449\n",
      "Epoch [8/20], Step [25/88], Loss: 0.2729\n",
      "Epoch [8/20], Step [26/88], Loss: 0.2614\n",
      "Epoch [8/20], Step [27/88], Loss: 0.1042\n",
      "Epoch [8/20], Step [28/88], Loss: 0.1715\n",
      "Epoch [8/20], Step [29/88], Loss: 0.1745\n",
      "Epoch [8/20], Step [30/88], Loss: 0.6421\n",
      "Epoch [8/20], Step [31/88], Loss: 0.3204\n",
      "Epoch [8/20], Step [32/88], Loss: 0.1631\n",
      "Epoch [8/20], Step [33/88], Loss: 0.2385\n",
      "Epoch [8/20], Step [34/88], Loss: 0.1027\n",
      "Epoch [8/20], Step [35/88], Loss: 0.2404\n",
      "Epoch [8/20], Step [36/88], Loss: 0.3665\n",
      "Epoch [8/20], Step [37/88], Loss: 0.1404\n",
      "Epoch [8/20], Step [38/88], Loss: 0.2394\n",
      "Epoch [8/20], Step [39/88], Loss: 0.1032\n",
      "Epoch [8/20], Step [40/88], Loss: 0.1492\n",
      "Epoch [8/20], Step [41/88], Loss: 0.2672\n",
      "Epoch [8/20], Step [42/88], Loss: 0.0532\n",
      "Epoch [8/20], Step [43/88], Loss: 0.1688\n",
      "Epoch [8/20], Step [44/88], Loss: 0.4396\n",
      "Epoch [8/20], Step [45/88], Loss: 0.1615\n",
      "Epoch [8/20], Step [46/88], Loss: 0.0950\n",
      "Epoch [8/20], Step [47/88], Loss: 0.1311\n",
      "Epoch [8/20], Step [48/88], Loss: 0.3325\n",
      "Epoch [8/20], Step [49/88], Loss: 0.1640\n",
      "Epoch [8/20], Step [50/88], Loss: 0.2171\n",
      "Epoch [8/20], Step [51/88], Loss: 0.2647\n",
      "Epoch [8/20], Step [52/88], Loss: 0.2040\n",
      "Epoch [8/20], Step [53/88], Loss: 0.1274\n",
      "Epoch [8/20], Step [54/88], Loss: 0.1627\n",
      "Epoch [8/20], Step [55/88], Loss: 0.1103\n",
      "Epoch [8/20], Step [56/88], Loss: 0.1632\n",
      "Epoch [8/20], Step [57/88], Loss: 0.1973\n",
      "Epoch [8/20], Step [58/88], Loss: 0.4021\n",
      "Epoch [8/20], Step [59/88], Loss: 0.1246\n",
      "Epoch [8/20], Step [60/88], Loss: 0.1405\n",
      "Epoch [8/20], Step [61/88], Loss: 0.0939\n",
      "Epoch [8/20], Step [62/88], Loss: 0.1536\n",
      "Epoch [8/20], Step [63/88], Loss: 0.0893\n",
      "Epoch [8/20], Step [64/88], Loss: 0.1711\n",
      "Epoch [8/20], Step [65/88], Loss: 0.2008\n",
      "Epoch [8/20], Step [66/88], Loss: 0.2362\n",
      "Epoch [8/20], Step [67/88], Loss: 0.1679\n",
      "Epoch [8/20], Step [68/88], Loss: 0.2135\n",
      "Epoch [8/20], Step [69/88], Loss: 0.2505\n",
      "Epoch [8/20], Step [70/88], Loss: 0.1996\n",
      "Epoch [8/20], Step [71/88], Loss: 0.3362\n",
      "Epoch [8/20], Step [72/88], Loss: 0.0637\n",
      "Epoch [8/20], Step [73/88], Loss: 0.0509\n",
      "Epoch [8/20], Step [74/88], Loss: 0.1637\n",
      "Epoch [8/20], Step [75/88], Loss: 0.2234\n",
      "Epoch [8/20], Step [76/88], Loss: 0.0787\n",
      "Epoch [8/20], Step [77/88], Loss: 0.1564\n",
      "Epoch [8/20], Step [78/88], Loss: 0.1604\n",
      "Epoch [8/20], Step [79/88], Loss: 0.1236\n",
      "Epoch [8/20], Step [80/88], Loss: 0.2281\n",
      "Epoch [8/20], Step [81/88], Loss: 0.0915\n",
      "Epoch [8/20], Step [82/88], Loss: 0.1199\n",
      "Epoch [8/20], Step [83/88], Loss: 0.3976\n",
      "Epoch [8/20], Step [84/88], Loss: 0.1266\n",
      "Epoch [8/20], Step [85/88], Loss: 0.2249\n",
      "Epoch [8/20], Step [86/88], Loss: 0.1497\n",
      "Epoch [8/20], Step [87/88], Loss: 0.3172\n",
      "Epoch [8/20], Step [88/88], Loss: 0.2437\n",
      "Accuracy at 8 epoch: 0.92%\n",
      "Epoch [9/20], Step [1/88], Loss: 0.1706\n",
      "Epoch [9/20], Step [2/88], Loss: 0.1633\n",
      "Epoch [9/20], Step [3/88], Loss: 0.2455\n",
      "Epoch [9/20], Step [4/88], Loss: 0.1469\n",
      "Epoch [9/20], Step [5/88], Loss: 0.3393\n",
      "Epoch [9/20], Step [6/88], Loss: 0.1735\n",
      "Epoch [9/20], Step [7/88], Loss: 0.1694\n",
      "Epoch [9/20], Step [8/88], Loss: 0.1254\n",
      "Epoch [9/20], Step [9/88], Loss: 0.1071\n",
      "Epoch [9/20], Step [10/88], Loss: 0.2645\n",
      "Epoch [9/20], Step [11/88], Loss: 0.0724\n",
      "Epoch [9/20], Step [12/88], Loss: 0.0655\n",
      "Epoch [9/20], Step [13/88], Loss: 0.0600\n",
      "Epoch [9/20], Step [14/88], Loss: 0.1762\n",
      "Epoch [9/20], Step [15/88], Loss: 0.0744\n",
      "Epoch [9/20], Step [16/88], Loss: 0.1820\n",
      "Epoch [9/20], Step [17/88], Loss: 0.1041\n",
      "Epoch [9/20], Step [18/88], Loss: 0.0548\n",
      "Epoch [9/20], Step [19/88], Loss: 0.1176\n",
      "Epoch [9/20], Step [20/88], Loss: 0.0772\n",
      "Epoch [9/20], Step [21/88], Loss: 0.2167\n",
      "Epoch [9/20], Step [22/88], Loss: 0.3591\n",
      "Epoch [9/20], Step [23/88], Loss: 0.1638\n",
      "Epoch [9/20], Step [24/88], Loss: 0.0636\n",
      "Epoch [9/20], Step [25/88], Loss: 0.1218\n",
      "Epoch [9/20], Step [26/88], Loss: 0.1505\n",
      "Epoch [9/20], Step [27/88], Loss: 0.2991\n",
      "Epoch [9/20], Step [28/88], Loss: 0.0790\n",
      "Epoch [9/20], Step [29/88], Loss: 0.1425\n",
      "Epoch [9/20], Step [30/88], Loss: 0.1098\n",
      "Epoch [9/20], Step [31/88], Loss: 0.0443\n",
      "Epoch [9/20], Step [32/88], Loss: 0.1598\n",
      "Epoch [9/20], Step [33/88], Loss: 0.2920\n",
      "Epoch [9/20], Step [34/88], Loss: 0.1690\n",
      "Epoch [9/20], Step [35/88], Loss: 0.1497\n",
      "Epoch [9/20], Step [36/88], Loss: 0.2909\n",
      "Epoch [9/20], Step [37/88], Loss: 0.1506\n",
      "Epoch [9/20], Step [38/88], Loss: 0.1651\n",
      "Epoch [9/20], Step [39/88], Loss: 0.1194\n",
      "Epoch [9/20], Step [40/88], Loss: 0.3375\n",
      "Epoch [9/20], Step [41/88], Loss: 0.4124\n",
      "Epoch [9/20], Step [42/88], Loss: 0.3854\n",
      "Epoch [9/20], Step [43/88], Loss: 0.2345\n",
      "Epoch [9/20], Step [44/88], Loss: 0.2340\n",
      "Epoch [9/20], Step [45/88], Loss: 0.1988\n",
      "Epoch [9/20], Step [46/88], Loss: 0.3031\n",
      "Epoch [9/20], Step [47/88], Loss: 0.0663\n",
      "Epoch [9/20], Step [48/88], Loss: 0.1812\n",
      "Epoch [9/20], Step [49/88], Loss: 0.0346\n",
      "Epoch [9/20], Step [50/88], Loss: 0.1274\n",
      "Epoch [9/20], Step [51/88], Loss: 0.0462\n",
      "Epoch [9/20], Step [52/88], Loss: 0.1293\n",
      "Epoch [9/20], Step [53/88], Loss: 0.3131\n",
      "Epoch [9/20], Step [54/88], Loss: 0.1160\n",
      "Epoch [9/20], Step [55/88], Loss: 0.0857\n",
      "Epoch [9/20], Step [56/88], Loss: 0.0541\n",
      "Epoch [9/20], Step [57/88], Loss: 0.2184\n",
      "Epoch [9/20], Step [58/88], Loss: 0.1364\n",
      "Epoch [9/20], Step [59/88], Loss: 0.1658\n",
      "Epoch [9/20], Step [60/88], Loss: 0.2017\n",
      "Epoch [9/20], Step [61/88], Loss: 0.2914\n",
      "Epoch [9/20], Step [62/88], Loss: 0.2161\n",
      "Epoch [9/20], Step [63/88], Loss: 0.0956\n",
      "Epoch [9/20], Step [64/88], Loss: 0.3669\n",
      "Epoch [9/20], Step [65/88], Loss: 0.1597\n",
      "Epoch [9/20], Step [66/88], Loss: 0.0647\n",
      "Epoch [9/20], Step [67/88], Loss: 0.2964\n",
      "Epoch [9/20], Step [68/88], Loss: 0.1992\n",
      "Epoch [9/20], Step [69/88], Loss: 0.1156\n",
      "Epoch [9/20], Step [70/88], Loss: 0.3217\n",
      "Epoch [9/20], Step [71/88], Loss: 0.1910\n",
      "Epoch [9/20], Step [72/88], Loss: 0.1721\n",
      "Epoch [9/20], Step [73/88], Loss: 0.1593\n",
      "Epoch [9/20], Step [74/88], Loss: 0.1076\n",
      "Epoch [9/20], Step [75/88], Loss: 0.1310\n",
      "Epoch [9/20], Step [76/88], Loss: 0.2012\n",
      "Epoch [9/20], Step [77/88], Loss: 0.1848\n",
      "Epoch [9/20], Step [78/88], Loss: 0.1019\n",
      "Epoch [9/20], Step [79/88], Loss: 0.2608\n",
      "Epoch [9/20], Step [80/88], Loss: 0.2068\n",
      "Epoch [9/20], Step [81/88], Loss: 0.2969\n",
      "Epoch [9/20], Step [82/88], Loss: 0.2853\n",
      "Epoch [9/20], Step [83/88], Loss: 0.4431\n",
      "Epoch [9/20], Step [84/88], Loss: 0.3751\n",
      "Epoch [9/20], Step [85/88], Loss: 0.2343\n",
      "Epoch [9/20], Step [86/88], Loss: 0.0306\n",
      "Epoch [9/20], Step [87/88], Loss: 0.2779\n",
      "Epoch [9/20], Step [88/88], Loss: 0.1263\n",
      "Accuracy at 9 epoch: 0.92%\n",
      "Epoch [10/20], Step [1/88], Loss: 0.1601\n",
      "Epoch [10/20], Step [2/88], Loss: 0.2348\n",
      "Epoch [10/20], Step [3/88], Loss: 0.2144\n",
      "Epoch [10/20], Step [4/88], Loss: 0.2246\n",
      "Epoch [10/20], Step [5/88], Loss: 0.0618\n",
      "Epoch [10/20], Step [6/88], Loss: 0.1432\n",
      "Epoch [10/20], Step [7/88], Loss: 0.2212\n",
      "Epoch [10/20], Step [8/88], Loss: 0.0715\n",
      "Epoch [10/20], Step [9/88], Loss: 0.2370\n",
      "Epoch [10/20], Step [10/88], Loss: 0.1661\n",
      "Epoch [10/20], Step [11/88], Loss: 0.0948\n",
      "Epoch [10/20], Step [12/88], Loss: 0.1139\n",
      "Epoch [10/20], Step [13/88], Loss: 0.1449\n",
      "Epoch [10/20], Step [14/88], Loss: 0.1763\n",
      "Epoch [10/20], Step [15/88], Loss: 0.1017\n",
      "Epoch [10/20], Step [16/88], Loss: 0.1050\n",
      "Epoch [10/20], Step [17/88], Loss: 0.0792\n",
      "Epoch [10/20], Step [18/88], Loss: 0.1437\n",
      "Epoch [10/20], Step [19/88], Loss: 0.1403\n",
      "Epoch [10/20], Step [20/88], Loss: 0.2280\n",
      "Epoch [10/20], Step [21/88], Loss: 0.0971\n",
      "Epoch [10/20], Step [22/88], Loss: 0.1972\n",
      "Epoch [10/20], Step [23/88], Loss: 0.1111\n",
      "Epoch [10/20], Step [24/88], Loss: 0.2767\n",
      "Epoch [10/20], Step [25/88], Loss: 0.2412\n",
      "Epoch [10/20], Step [26/88], Loss: 0.1749\n",
      "Epoch [10/20], Step [27/88], Loss: 0.2325\n",
      "Epoch [10/20], Step [28/88], Loss: 0.0566\n",
      "Epoch [10/20], Step [29/88], Loss: 0.1601\n",
      "Epoch [10/20], Step [30/88], Loss: 0.2952\n",
      "Epoch [10/20], Step [31/88], Loss: 0.2458\n",
      "Epoch [10/20], Step [32/88], Loss: 0.2530\n",
      "Epoch [10/20], Step [33/88], Loss: 0.2628\n",
      "Epoch [10/20], Step [34/88], Loss: 0.0683\n",
      "Epoch [10/20], Step [35/88], Loss: 0.3199\n",
      "Epoch [10/20], Step [36/88], Loss: 0.2145\n",
      "Epoch [10/20], Step [37/88], Loss: 0.2391\n",
      "Epoch [10/20], Step [38/88], Loss: 0.2587\n",
      "Epoch [10/20], Step [39/88], Loss: 0.1740\n",
      "Epoch [10/20], Step [40/88], Loss: 0.1064\n",
      "Epoch [10/20], Step [41/88], Loss: 0.2466\n",
      "Epoch [10/20], Step [42/88], Loss: 0.1369\n",
      "Epoch [10/20], Step [43/88], Loss: 0.1649\n",
      "Epoch [10/20], Step [44/88], Loss: 0.1876\n",
      "Epoch [10/20], Step [45/88], Loss: 0.1842\n",
      "Epoch [10/20], Step [46/88], Loss: 0.1356\n",
      "Epoch [10/20], Step [47/88], Loss: 0.0894\n",
      "Epoch [10/20], Step [48/88], Loss: 0.1723\n",
      "Epoch [10/20], Step [49/88], Loss: 0.0803\n",
      "Epoch [10/20], Step [50/88], Loss: 0.1076\n",
      "Epoch [10/20], Step [51/88], Loss: 0.2154\n",
      "Epoch [10/20], Step [52/88], Loss: 0.0433\n",
      "Epoch [10/20], Step [53/88], Loss: 0.2851\n",
      "Epoch [10/20], Step [54/88], Loss: 0.0868\n",
      "Epoch [10/20], Step [55/88], Loss: 0.2366\n",
      "Epoch [10/20], Step [56/88], Loss: 0.1753\n",
      "Epoch [10/20], Step [57/88], Loss: 0.0971\n",
      "Epoch [10/20], Step [58/88], Loss: 0.1491\n",
      "Epoch [10/20], Step [59/88], Loss: 0.2612\n",
      "Epoch [10/20], Step [60/88], Loss: 0.0674\n",
      "Epoch [10/20], Step [61/88], Loss: 0.1542\n",
      "Epoch [10/20], Step [62/88], Loss: 0.1218\n",
      "Epoch [10/20], Step [63/88], Loss: 0.0557\n",
      "Epoch [10/20], Step [64/88], Loss: 0.0526\n",
      "Epoch [10/20], Step [65/88], Loss: 0.1396\n",
      "Epoch [10/20], Step [66/88], Loss: 0.2081\n",
      "Epoch [10/20], Step [67/88], Loss: 0.1734\n",
      "Epoch [10/20], Step [68/88], Loss: 0.0683\n",
      "Epoch [10/20], Step [69/88], Loss: 0.2677\n",
      "Epoch [10/20], Step [70/88], Loss: 0.0983\n",
      "Epoch [10/20], Step [71/88], Loss: 0.0721\n",
      "Epoch [10/20], Step [72/88], Loss: 0.1251\n",
      "Epoch [10/20], Step [73/88], Loss: 0.1319\n",
      "Epoch [10/20], Step [74/88], Loss: 0.2764\n",
      "Epoch [10/20], Step [75/88], Loss: 0.1058\n",
      "Epoch [10/20], Step [76/88], Loss: 0.0929\n",
      "Epoch [10/20], Step [77/88], Loss: 0.0870\n",
      "Epoch [10/20], Step [78/88], Loss: 0.2079\n",
      "Epoch [10/20], Step [79/88], Loss: 0.1285\n",
      "Epoch [10/20], Step [80/88], Loss: 0.0487\n",
      "Epoch [10/20], Step [81/88], Loss: 0.0963\n",
      "Epoch [10/20], Step [82/88], Loss: 0.1333\n",
      "Epoch [10/20], Step [83/88], Loss: 0.1394\n",
      "Epoch [10/20], Step [84/88], Loss: 0.1593\n",
      "Epoch [10/20], Step [85/88], Loss: 0.1241\n",
      "Epoch [10/20], Step [86/88], Loss: 0.1631\n",
      "Epoch [10/20], Step [87/88], Loss: 0.1578\n",
      "Epoch [10/20], Step [88/88], Loss: 0.2363\n",
      "Accuracy at 10 epoch: 0.94%\n",
      "Epoch [11/20], Step [1/88], Loss: 0.1931\n",
      "Epoch [11/20], Step [2/88], Loss: 0.2558\n",
      "Epoch [11/20], Step [3/88], Loss: 0.0420\n",
      "Epoch [11/20], Step [4/88], Loss: 0.1399\n",
      "Epoch [11/20], Step [5/88], Loss: 0.1092\n",
      "Epoch [11/20], Step [6/88], Loss: 0.0622\n",
      "Epoch [11/20], Step [7/88], Loss: 0.1547\n",
      "Epoch [11/20], Step [8/88], Loss: 0.0545\n",
      "Epoch [11/20], Step [9/88], Loss: 0.1251\n",
      "Epoch [11/20], Step [10/88], Loss: 0.1073\n",
      "Epoch [11/20], Step [11/88], Loss: 0.0977\n",
      "Epoch [11/20], Step [12/88], Loss: 0.0609\n",
      "Epoch [11/20], Step [13/88], Loss: 0.1020\n",
      "Epoch [11/20], Step [14/88], Loss: 0.1756\n",
      "Epoch [11/20], Step [15/88], Loss: 0.0984\n",
      "Epoch [11/20], Step [16/88], Loss: 0.0343\n",
      "Epoch [11/20], Step [17/88], Loss: 0.0726\n",
      "Epoch [11/20], Step [18/88], Loss: 0.2209\n",
      "Epoch [11/20], Step [19/88], Loss: 0.1359\n",
      "Epoch [11/20], Step [20/88], Loss: 0.2425\n",
      "Epoch [11/20], Step [21/88], Loss: 0.1923\n",
      "Epoch [11/20], Step [22/88], Loss: 0.0477\n",
      "Epoch [11/20], Step [23/88], Loss: 0.0841\n",
      "Epoch [11/20], Step [24/88], Loss: 0.1263\n",
      "Epoch [11/20], Step [25/88], Loss: 0.0333\n",
      "Epoch [11/20], Step [26/88], Loss: 0.2687\n",
      "Epoch [11/20], Step [27/88], Loss: 0.0704\n",
      "Epoch [11/20], Step [28/88], Loss: 0.0762\n",
      "Epoch [11/20], Step [29/88], Loss: 0.0229\n",
      "Epoch [11/20], Step [30/88], Loss: 0.0246\n",
      "Epoch [11/20], Step [31/88], Loss: 0.0542\n",
      "Epoch [11/20], Step [32/88], Loss: 0.2507\n",
      "Epoch [11/20], Step [33/88], Loss: 0.1167\n",
      "Epoch [11/20], Step [34/88], Loss: 0.0293\n",
      "Epoch [11/20], Step [35/88], Loss: 0.1353\n",
      "Epoch [11/20], Step [36/88], Loss: 0.1051\n",
      "Epoch [11/20], Step [37/88], Loss: 0.3222\n",
      "Epoch [11/20], Step [38/88], Loss: 0.1954\n",
      "Epoch [11/20], Step [39/88], Loss: 0.1119\n",
      "Epoch [11/20], Step [40/88], Loss: 0.1471\n",
      "Epoch [11/20], Step [41/88], Loss: 0.1016\n",
      "Epoch [11/20], Step [42/88], Loss: 0.2090\n",
      "Epoch [11/20], Step [43/88], Loss: 0.1577\n",
      "Epoch [11/20], Step [44/88], Loss: 0.0678\n",
      "Epoch [11/20], Step [45/88], Loss: 0.1178\n",
      "Epoch [11/20], Step [46/88], Loss: 0.0848\n",
      "Epoch [11/20], Step [47/88], Loss: 0.3871\n",
      "Epoch [11/20], Step [48/88], Loss: 0.1695\n",
      "Epoch [11/20], Step [49/88], Loss: 0.1231\n",
      "Epoch [11/20], Step [50/88], Loss: 0.1145\n",
      "Epoch [11/20], Step [51/88], Loss: 0.2720\n",
      "Epoch [11/20], Step [52/88], Loss: 0.1634\n",
      "Epoch [11/20], Step [53/88], Loss: 0.0760\n",
      "Epoch [11/20], Step [54/88], Loss: 0.1802\n",
      "Epoch [11/20], Step [55/88], Loss: 0.0810\n",
      "Epoch [11/20], Step [56/88], Loss: 0.1617\n",
      "Epoch [11/20], Step [57/88], Loss: 0.0710\n",
      "Epoch [11/20], Step [58/88], Loss: 0.2459\n",
      "Epoch [11/20], Step [59/88], Loss: 0.1633\n",
      "Epoch [11/20], Step [60/88], Loss: 0.1535\n",
      "Epoch [11/20], Step [61/88], Loss: 0.0348\n",
      "Epoch [11/20], Step [62/88], Loss: 0.0855\n",
      "Epoch [11/20], Step [63/88], Loss: 0.2450\n",
      "Epoch [11/20], Step [64/88], Loss: 0.1419\n",
      "Epoch [11/20], Step [65/88], Loss: 0.1064\n",
      "Epoch [11/20], Step [66/88], Loss: 0.0626\n",
      "Epoch [11/20], Step [67/88], Loss: 0.0276\n",
      "Epoch [11/20], Step [68/88], Loss: 0.0556\n",
      "Epoch [11/20], Step [69/88], Loss: 0.0919\n",
      "Epoch [11/20], Step [70/88], Loss: 0.0743\n",
      "Epoch [11/20], Step [71/88], Loss: 0.0690\n",
      "Epoch [11/20], Step [72/88], Loss: 0.0577\n",
      "Epoch [11/20], Step [73/88], Loss: 0.1942\n",
      "Epoch [11/20], Step [74/88], Loss: 0.0895\n",
      "Epoch [11/20], Step [75/88], Loss: 0.1306\n",
      "Epoch [11/20], Step [76/88], Loss: 0.4321\n",
      "Epoch [11/20], Step [77/88], Loss: 0.0694\n",
      "Epoch [11/20], Step [78/88], Loss: 0.3966\n",
      "Epoch [11/20], Step [79/88], Loss: 0.2137\n",
      "Epoch [11/20], Step [80/88], Loss: 0.0861\n",
      "Epoch [11/20], Step [81/88], Loss: 0.1478\n",
      "Epoch [11/20], Step [82/88], Loss: 0.0706\n",
      "Epoch [11/20], Step [83/88], Loss: 0.1658\n",
      "Epoch [11/20], Step [84/88], Loss: 0.0489\n",
      "Epoch [11/20], Step [85/88], Loss: 0.1235\n",
      "Epoch [11/20], Step [86/88], Loss: 0.1438\n",
      "Epoch [11/20], Step [87/88], Loss: 0.0782\n",
      "Epoch [11/20], Step [88/88], Loss: 0.3784\n",
      "Accuracy at 11 epoch: 0.94%\n",
      "Epoch [12/20], Step [1/88], Loss: 0.2180\n",
      "Epoch [12/20], Step [2/88], Loss: 0.1167\n",
      "Epoch [12/20], Step [3/88], Loss: 0.0180\n",
      "Epoch [12/20], Step [4/88], Loss: 0.1317\n",
      "Epoch [12/20], Step [5/88], Loss: 0.0626\n",
      "Epoch [12/20], Step [6/88], Loss: 0.0645\n",
      "Epoch [12/20], Step [7/88], Loss: 0.0582\n",
      "Epoch [12/20], Step [8/88], Loss: 0.0940\n",
      "Epoch [12/20], Step [9/88], Loss: 0.0915\n",
      "Epoch [12/20], Step [10/88], Loss: 0.1720\n",
      "Epoch [12/20], Step [11/88], Loss: 0.1400\n",
      "Epoch [12/20], Step [12/88], Loss: 0.0590\n",
      "Epoch [12/20], Step [13/88], Loss: 0.0979\n",
      "Epoch [12/20], Step [14/88], Loss: 0.0943\n",
      "Epoch [12/20], Step [15/88], Loss: 0.1247\n",
      "Epoch [12/20], Step [16/88], Loss: 0.0237\n",
      "Epoch [12/20], Step [17/88], Loss: 0.1240\n",
      "Epoch [12/20], Step [18/88], Loss: 0.0269\n",
      "Epoch [12/20], Step [19/88], Loss: 0.0831\n",
      "Epoch [12/20], Step [20/88], Loss: 0.0827\n",
      "Epoch [12/20], Step [21/88], Loss: 0.0239\n",
      "Epoch [12/20], Step [22/88], Loss: 0.0244\n",
      "Epoch [12/20], Step [23/88], Loss: 0.0551\n",
      "Epoch [12/20], Step [24/88], Loss: 0.1654\n",
      "Epoch [12/20], Step [25/88], Loss: 0.2075\n",
      "Epoch [12/20], Step [26/88], Loss: 0.0516\n",
      "Epoch [12/20], Step [27/88], Loss: 0.1616\n",
      "Epoch [12/20], Step [28/88], Loss: 0.0763\n",
      "Epoch [12/20], Step [29/88], Loss: 0.0531\n",
      "Epoch [12/20], Step [30/88], Loss: 0.0850\n",
      "Epoch [12/20], Step [31/88], Loss: 0.0156\n",
      "Epoch [12/20], Step [32/88], Loss: 0.0402\n",
      "Epoch [12/20], Step [33/88], Loss: 0.1993\n",
      "Epoch [12/20], Step [34/88], Loss: 0.1451\n",
      "Epoch [12/20], Step [35/88], Loss: 0.1013\n",
      "Epoch [12/20], Step [36/88], Loss: 0.0113\n",
      "Epoch [12/20], Step [37/88], Loss: 0.0851\n",
      "Epoch [12/20], Step [38/88], Loss: 0.1106\n",
      "Epoch [12/20], Step [39/88], Loss: 0.1388\n",
      "Epoch [12/20], Step [40/88], Loss: 0.0750\n",
      "Epoch [12/20], Step [41/88], Loss: 0.0751\n",
      "Epoch [12/20], Step [42/88], Loss: 0.0535\n",
      "Epoch [12/20], Step [43/88], Loss: 0.1223\n",
      "Epoch [12/20], Step [44/88], Loss: 0.2710\n",
      "Epoch [12/20], Step [45/88], Loss: 0.0414\n",
      "Epoch [12/20], Step [46/88], Loss: 0.0968\n",
      "Epoch [12/20], Step [47/88], Loss: 0.0881\n",
      "Epoch [12/20], Step [48/88], Loss: 0.0352\n",
      "Epoch [12/20], Step [49/88], Loss: 0.0601\n",
      "Epoch [12/20], Step [50/88], Loss: 0.0933\n",
      "Epoch [12/20], Step [51/88], Loss: 0.1189\n",
      "Epoch [12/20], Step [52/88], Loss: 0.0536\n",
      "Epoch [12/20], Step [53/88], Loss: 0.1722\n",
      "Epoch [12/20], Step [54/88], Loss: 0.0391\n",
      "Epoch [12/20], Step [55/88], Loss: 0.1458\n",
      "Epoch [12/20], Step [56/88], Loss: 0.2310\n",
      "Epoch [12/20], Step [57/88], Loss: 0.2101\n",
      "Epoch [12/20], Step [58/88], Loss: 0.0608\n",
      "Epoch [12/20], Step [59/88], Loss: 0.1831\n",
      "Epoch [12/20], Step [60/88], Loss: 0.1031\n",
      "Epoch [12/20], Step [61/88], Loss: 0.1418\n",
      "Epoch [12/20], Step [62/88], Loss: 0.0797\n",
      "Epoch [12/20], Step [63/88], Loss: 0.0875\n",
      "Epoch [12/20], Step [64/88], Loss: 0.0684\n",
      "Epoch [12/20], Step [65/88], Loss: 0.1741\n",
      "Epoch [12/20], Step [66/88], Loss: 0.1207\n",
      "Epoch [12/20], Step [67/88], Loss: 0.0771\n",
      "Epoch [12/20], Step [68/88], Loss: 0.0305\n",
      "Epoch [12/20], Step [69/88], Loss: 0.1279\n",
      "Epoch [12/20], Step [70/88], Loss: 0.2832\n",
      "Epoch [12/20], Step [71/88], Loss: 0.1031\n",
      "Epoch [12/20], Step [72/88], Loss: 0.0793\n",
      "Epoch [12/20], Step [73/88], Loss: 0.2061\n",
      "Epoch [12/20], Step [74/88], Loss: 0.1513\n",
      "Epoch [12/20], Step [75/88], Loss: 0.0617\n",
      "Epoch [12/20], Step [76/88], Loss: 0.1338\n",
      "Epoch [12/20], Step [77/88], Loss: 0.0949\n",
      "Epoch [12/20], Step [78/88], Loss: 0.0801\n",
      "Epoch [12/20], Step [79/88], Loss: 0.1129\n",
      "Epoch [12/20], Step [80/88], Loss: 0.1298\n",
      "Epoch [12/20], Step [81/88], Loss: 0.1426\n",
      "Epoch [12/20], Step [82/88], Loss: 0.1576\n",
      "Epoch [12/20], Step [83/88], Loss: 0.0996\n",
      "Epoch [12/20], Step [84/88], Loss: 0.1628\n",
      "Epoch [12/20], Step [85/88], Loss: 0.0918\n",
      "Epoch [12/20], Step [86/88], Loss: 0.1386\n",
      "Epoch [12/20], Step [87/88], Loss: 0.2930\n",
      "Epoch [12/20], Step [88/88], Loss: 0.0360\n",
      "Accuracy at 12 epoch: 0.95%\n",
      "Epoch [13/20], Step [1/88], Loss: 0.0340\n",
      "Epoch [13/20], Step [2/88], Loss: 0.0240\n",
      "Epoch [13/20], Step [3/88], Loss: 0.0511\n",
      "Epoch [13/20], Step [4/88], Loss: 0.0730\n",
      "Epoch [13/20], Step [5/88], Loss: 0.0356\n",
      "Epoch [13/20], Step [6/88], Loss: 0.1133\n",
      "Epoch [13/20], Step [7/88], Loss: 0.2155\n",
      "Epoch [13/20], Step [8/88], Loss: 0.0466\n",
      "Epoch [13/20], Step [9/88], Loss: 0.0388\n",
      "Epoch [13/20], Step [10/88], Loss: 0.1621\n",
      "Epoch [13/20], Step [11/88], Loss: 0.0297\n",
      "Epoch [13/20], Step [12/88], Loss: 0.0763\n",
      "Epoch [13/20], Step [13/88], Loss: 0.3050\n",
      "Epoch [13/20], Step [14/88], Loss: 0.0414\n",
      "Epoch [13/20], Step [15/88], Loss: 0.0997\n",
      "Epoch [13/20], Step [16/88], Loss: 0.1656\n",
      "Epoch [13/20], Step [17/88], Loss: 0.0842\n",
      "Epoch [13/20], Step [18/88], Loss: 0.0379\n",
      "Epoch [13/20], Step [19/88], Loss: 0.0787\n",
      "Epoch [13/20], Step [20/88], Loss: 0.1298\n",
      "Epoch [13/20], Step [21/88], Loss: 0.0450\n",
      "Epoch [13/20], Step [22/88], Loss: 0.0986\n",
      "Epoch [13/20], Step [23/88], Loss: 0.1398\n",
      "Epoch [13/20], Step [24/88], Loss: 0.0744\n",
      "Epoch [13/20], Step [25/88], Loss: 0.1800\n",
      "Epoch [13/20], Step [26/88], Loss: 0.0506\n",
      "Epoch [13/20], Step [27/88], Loss: 0.0914\n",
      "Epoch [13/20], Step [28/88], Loss: 0.0617\n",
      "Epoch [13/20], Step [29/88], Loss: 0.0340\n",
      "Epoch [13/20], Step [30/88], Loss: 0.1659\n",
      "Epoch [13/20], Step [31/88], Loss: 0.0705\n",
      "Epoch [13/20], Step [32/88], Loss: 0.0240\n",
      "Epoch [13/20], Step [33/88], Loss: 0.0799\n",
      "Epoch [13/20], Step [34/88], Loss: 0.0452\n",
      "Epoch [13/20], Step [35/88], Loss: 0.1515\n",
      "Epoch [13/20], Step [36/88], Loss: 0.0492\n",
      "Epoch [13/20], Step [37/88], Loss: 0.0742\n",
      "Epoch [13/20], Step [38/88], Loss: 0.0561\n",
      "Epoch [13/20], Step [39/88], Loss: 0.1287\n",
      "Epoch [13/20], Step [40/88], Loss: 0.1947\n",
      "Epoch [13/20], Step [41/88], Loss: 0.1254\n",
      "Epoch [13/20], Step [42/88], Loss: 0.0491\n",
      "Epoch [13/20], Step [43/88], Loss: 0.1713\n",
      "Epoch [13/20], Step [44/88], Loss: 0.0186\n",
      "Epoch [13/20], Step [45/88], Loss: 0.0210\n",
      "Epoch [13/20], Step [46/88], Loss: 0.1637\n",
      "Epoch [13/20], Step [47/88], Loss: 0.1181\n",
      "Epoch [13/20], Step [48/88], Loss: 0.1516\n",
      "Epoch [13/20], Step [49/88], Loss: 0.0907\n",
      "Epoch [13/20], Step [50/88], Loss: 0.1765\n",
      "Epoch [13/20], Step [51/88], Loss: 0.1001\n",
      "Epoch [13/20], Step [52/88], Loss: 0.0578\n",
      "Epoch [13/20], Step [53/88], Loss: 0.1221\n",
      "Epoch [13/20], Step [54/88], Loss: 0.1477\n",
      "Epoch [13/20], Step [55/88], Loss: 0.1050\n",
      "Epoch [13/20], Step [56/88], Loss: 0.0327\n",
      "Epoch [13/20], Step [57/88], Loss: 0.2490\n",
      "Epoch [13/20], Step [58/88], Loss: 0.1580\n",
      "Epoch [13/20], Step [59/88], Loss: 0.0745\n",
      "Epoch [13/20], Step [60/88], Loss: 0.0872\n",
      "Epoch [13/20], Step [61/88], Loss: 0.2409\n",
      "Epoch [13/20], Step [62/88], Loss: 0.0985\n",
      "Epoch [13/20], Step [63/88], Loss: 0.2187\n",
      "Epoch [13/20], Step [64/88], Loss: 0.1149\n",
      "Epoch [13/20], Step [65/88], Loss: 0.0925\n",
      "Epoch [13/20], Step [66/88], Loss: 0.0937\n",
      "Epoch [13/20], Step [67/88], Loss: 0.1254\n",
      "Epoch [13/20], Step [68/88], Loss: 0.1267\n",
      "Epoch [13/20], Step [69/88], Loss: 0.1486\n",
      "Epoch [13/20], Step [70/88], Loss: 0.0993\n",
      "Epoch [13/20], Step [71/88], Loss: 0.0511\n",
      "Epoch [13/20], Step [72/88], Loss: 0.1549\n",
      "Epoch [13/20], Step [73/88], Loss: 0.2472\n",
      "Epoch [13/20], Step [74/88], Loss: 0.0464\n",
      "Epoch [13/20], Step [75/88], Loss: 0.0833\n",
      "Epoch [13/20], Step [76/88], Loss: 0.1365\n",
      "Epoch [13/20], Step [77/88], Loss: 0.0536\n",
      "Epoch [13/20], Step [78/88], Loss: 0.0797\n",
      "Epoch [13/20], Step [79/88], Loss: 0.0804\n",
      "Epoch [13/20], Step [80/88], Loss: 0.1349\n",
      "Epoch [13/20], Step [81/88], Loss: 0.1276\n",
      "Epoch [13/20], Step [82/88], Loss: 0.0619\n",
      "Epoch [13/20], Step [83/88], Loss: 0.0946\n",
      "Epoch [13/20], Step [84/88], Loss: 0.0671\n",
      "Epoch [13/20], Step [85/88], Loss: 0.0941\n",
      "Epoch [13/20], Step [86/88], Loss: 0.1562\n",
      "Epoch [13/20], Step [87/88], Loss: 0.0510\n",
      "Epoch [13/20], Step [88/88], Loss: 0.0304\n",
      "Accuracy at 13 epoch: 0.96%\n",
      "Epoch [14/20], Step [1/88], Loss: 0.0470\n",
      "Epoch [14/20], Step [2/88], Loss: 0.1543\n",
      "Epoch [14/20], Step [3/88], Loss: 0.2173\n",
      "Epoch [14/20], Step [4/88], Loss: 0.0681\n",
      "Epoch [14/20], Step [5/88], Loss: 0.0379\n",
      "Epoch [14/20], Step [6/88], Loss: 0.0955\n",
      "Epoch [14/20], Step [7/88], Loss: 0.0290\n",
      "Epoch [14/20], Step [8/88], Loss: 0.1851\n",
      "Epoch [14/20], Step [9/88], Loss: 0.0593\n",
      "Epoch [14/20], Step [10/88], Loss: 0.0344\n",
      "Epoch [14/20], Step [11/88], Loss: 0.2872\n",
      "Epoch [14/20], Step [12/88], Loss: 0.1195\n",
      "Epoch [14/20], Step [13/88], Loss: 0.0156\n",
      "Epoch [14/20], Step [14/88], Loss: 0.0354\n",
      "Epoch [14/20], Step [15/88], Loss: 0.0653\n",
      "Epoch [14/20], Step [16/88], Loss: 0.0262\n",
      "Epoch [14/20], Step [17/88], Loss: 0.1406\n",
      "Epoch [14/20], Step [18/88], Loss: 0.0667\n",
      "Epoch [14/20], Step [19/88], Loss: 0.0432\n",
      "Epoch [14/20], Step [20/88], Loss: 0.0935\n",
      "Epoch [14/20], Step [21/88], Loss: 0.0380\n",
      "Epoch [14/20], Step [22/88], Loss: 0.0396\n",
      "Epoch [14/20], Step [23/88], Loss: 0.0666\n",
      "Epoch [14/20], Step [24/88], Loss: 0.0346\n",
      "Epoch [14/20], Step [25/88], Loss: 0.0359\n",
      "Epoch [14/20], Step [26/88], Loss: 0.0822\n",
      "Epoch [14/20], Step [27/88], Loss: 0.0566\n",
      "Epoch [14/20], Step [28/88], Loss: 0.0659\n",
      "Epoch [14/20], Step [29/88], Loss: 0.1517\n",
      "Epoch [14/20], Step [30/88], Loss: 0.1270\n",
      "Epoch [14/20], Step [31/88], Loss: 0.0479\n",
      "Epoch [14/20], Step [32/88], Loss: 0.0608\n",
      "Epoch [14/20], Step [33/88], Loss: 0.1319\n",
      "Epoch [14/20], Step [34/88], Loss: 0.0533\n",
      "Epoch [14/20], Step [35/88], Loss: 0.0186\n",
      "Epoch [14/20], Step [36/88], Loss: 0.1173\n",
      "Epoch [14/20], Step [37/88], Loss: 0.1664\n",
      "Epoch [14/20], Step [38/88], Loss: 0.1667\n",
      "Epoch [14/20], Step [39/88], Loss: 0.0327\n",
      "Epoch [14/20], Step [40/88], Loss: 0.0809\n",
      "Epoch [14/20], Step [41/88], Loss: 0.0174\n",
      "Epoch [14/20], Step [42/88], Loss: 0.0319\n",
      "Epoch [14/20], Step [43/88], Loss: 0.0957\n",
      "Epoch [14/20], Step [44/88], Loss: 0.1868\n",
      "Epoch [14/20], Step [45/88], Loss: 0.1802\n",
      "Epoch [14/20], Step [46/88], Loss: 0.0374\n",
      "Epoch [14/20], Step [47/88], Loss: 0.1385\n",
      "Epoch [14/20], Step [48/88], Loss: 0.2398\n",
      "Epoch [14/20], Step [49/88], Loss: 0.0278\n",
      "Epoch [14/20], Step [50/88], Loss: 0.0661\n",
      "Epoch [14/20], Step [51/88], Loss: 0.0324\n",
      "Epoch [14/20], Step [52/88], Loss: 0.1015\n",
      "Epoch [14/20], Step [53/88], Loss: 0.4124\n",
      "Epoch [14/20], Step [54/88], Loss: 0.0483\n",
      "Epoch [14/20], Step [55/88], Loss: 0.0751\n",
      "Epoch [14/20], Step [56/88], Loss: 0.0418\n",
      "Epoch [14/20], Step [57/88], Loss: 0.0429\n",
      "Epoch [14/20], Step [58/88], Loss: 0.1816\n",
      "Epoch [14/20], Step [59/88], Loss: 0.0554\n",
      "Epoch [14/20], Step [60/88], Loss: 0.0663\n",
      "Epoch [14/20], Step [61/88], Loss: 0.0262\n",
      "Epoch [14/20], Step [62/88], Loss: 0.0619\n",
      "Epoch [14/20], Step [63/88], Loss: 0.1905\n",
      "Epoch [14/20], Step [64/88], Loss: 0.0568\n",
      "Epoch [14/20], Step [65/88], Loss: 0.0950\n",
      "Epoch [14/20], Step [66/88], Loss: 0.1210\n",
      "Epoch [14/20], Step [67/88], Loss: 0.0458\n",
      "Epoch [14/20], Step [68/88], Loss: 0.1058\n",
      "Epoch [14/20], Step [69/88], Loss: 0.1138\n",
      "Epoch [14/20], Step [70/88], Loss: 0.0553\n",
      "Epoch [14/20], Step [71/88], Loss: 0.0439\n",
      "Epoch [14/20], Step [72/88], Loss: 0.0823\n",
      "Epoch [14/20], Step [73/88], Loss: 0.1114\n",
      "Epoch [14/20], Step [74/88], Loss: 0.0217\n",
      "Epoch [14/20], Step [75/88], Loss: 0.2361\n",
      "Epoch [14/20], Step [76/88], Loss: 0.1659\n",
      "Epoch [14/20], Step [77/88], Loss: 0.0679\n",
      "Epoch [14/20], Step [78/88], Loss: 0.0932\n",
      "Epoch [14/20], Step [79/88], Loss: 0.0948\n",
      "Epoch [14/20], Step [80/88], Loss: 0.1245\n",
      "Epoch [14/20], Step [81/88], Loss: 0.1223\n",
      "Epoch [14/20], Step [82/88], Loss: 0.0773\n",
      "Epoch [14/20], Step [83/88], Loss: 0.5228\n",
      "Epoch [14/20], Step [84/88], Loss: 0.1184\n",
      "Epoch [14/20], Step [85/88], Loss: 0.2900\n",
      "Epoch [14/20], Step [86/88], Loss: 0.0179\n",
      "Epoch [14/20], Step [87/88], Loss: 0.0370\n",
      "Epoch [14/20], Step [88/88], Loss: 0.0413\n",
      "Accuracy at 14 epoch: 0.96%\n",
      "Epoch [15/20], Step [1/88], Loss: 0.2786\n",
      "Epoch [15/20], Step [2/88], Loss: 0.0752\n",
      "Epoch [15/20], Step [3/88], Loss: 0.0493\n",
      "Epoch [15/20], Step [4/88], Loss: 0.0146\n",
      "Epoch [15/20], Step [5/88], Loss: 0.1318\n",
      "Epoch [15/20], Step [6/88], Loss: 0.0492\n",
      "Epoch [15/20], Step [7/88], Loss: 0.0407\n",
      "Epoch [15/20], Step [8/88], Loss: 0.0482\n",
      "Epoch [15/20], Step [9/88], Loss: 0.0327\n",
      "Epoch [15/20], Step [10/88], Loss: 0.1212\n",
      "Epoch [15/20], Step [11/88], Loss: 0.0813\n",
      "Epoch [15/20], Step [12/88], Loss: 0.1064\n",
      "Epoch [15/20], Step [13/88], Loss: 0.0498\n",
      "Epoch [15/20], Step [14/88], Loss: 0.0131\n",
      "Epoch [15/20], Step [15/88], Loss: 0.1602\n",
      "Epoch [15/20], Step [16/88], Loss: 0.0978\n",
      "Epoch [15/20], Step [17/88], Loss: 0.1256\n",
      "Epoch [15/20], Step [18/88], Loss: 0.1714\n",
      "Epoch [15/20], Step [19/88], Loss: 0.0945\n",
      "Epoch [15/20], Step [20/88], Loss: 0.0309\n",
      "Epoch [15/20], Step [21/88], Loss: 0.2162\n",
      "Epoch [15/20], Step [22/88], Loss: 0.0956\n",
      "Epoch [15/20], Step [23/88], Loss: 0.0380\n",
      "Epoch [15/20], Step [24/88], Loss: 0.0581\n",
      "Epoch [15/20], Step [25/88], Loss: 0.0503\n",
      "Epoch [15/20], Step [26/88], Loss: 0.0401\n",
      "Epoch [15/20], Step [27/88], Loss: 0.0309\n",
      "Epoch [15/20], Step [28/88], Loss: 0.0407\n",
      "Epoch [15/20], Step [29/88], Loss: 0.0577\n",
      "Epoch [15/20], Step [30/88], Loss: 0.0246\n",
      "Epoch [15/20], Step [31/88], Loss: 0.0533\n",
      "Epoch [15/20], Step [32/88], Loss: 0.0517\n",
      "Epoch [15/20], Step [33/88], Loss: 0.0588\n",
      "Epoch [15/20], Step [34/88], Loss: 0.1493\n",
      "Epoch [15/20], Step [35/88], Loss: 0.0521\n",
      "Epoch [15/20], Step [36/88], Loss: 0.0582\n",
      "Epoch [15/20], Step [37/88], Loss: 0.0581\n",
      "Epoch [15/20], Step [38/88], Loss: 0.1753\n",
      "Epoch [15/20], Step [39/88], Loss: 0.1135\n",
      "Epoch [15/20], Step [40/88], Loss: 0.0617\n",
      "Epoch [15/20], Step [41/88], Loss: 0.1122\n",
      "Epoch [15/20], Step [42/88], Loss: 0.1550\n",
      "Epoch [15/20], Step [43/88], Loss: 0.0291\n",
      "Epoch [15/20], Step [44/88], Loss: 0.0550\n",
      "Epoch [15/20], Step [45/88], Loss: 0.0757\n",
      "Epoch [15/20], Step [46/88], Loss: 0.0419\n",
      "Epoch [15/20], Step [47/88], Loss: 0.0191\n",
      "Epoch [15/20], Step [48/88], Loss: 0.0754\n",
      "Epoch [15/20], Step [49/88], Loss: 0.0828\n",
      "Epoch [15/20], Step [50/88], Loss: 0.0957\n",
      "Epoch [15/20], Step [51/88], Loss: 0.1561\n",
      "Epoch [15/20], Step [52/88], Loss: 0.0544\n",
      "Epoch [15/20], Step [53/88], Loss: 0.1609\n",
      "Epoch [15/20], Step [54/88], Loss: 0.0663\n",
      "Epoch [15/20], Step [55/88], Loss: 0.1117\n",
      "Epoch [15/20], Step [56/88], Loss: 0.0387\n",
      "Epoch [15/20], Step [57/88], Loss: 0.0162\n",
      "Epoch [15/20], Step [58/88], Loss: 0.1762\n",
      "Epoch [15/20], Step [59/88], Loss: 0.0694\n",
      "Epoch [15/20], Step [60/88], Loss: 0.0680\n",
      "Epoch [15/20], Step [61/88], Loss: 0.3125\n",
      "Epoch [15/20], Step [62/88], Loss: 0.0211\n",
      "Epoch [15/20], Step [63/88], Loss: 0.0208\n",
      "Epoch [15/20], Step [64/88], Loss: 0.0059\n",
      "Epoch [15/20], Step [65/88], Loss: 0.0899\n",
      "Epoch [15/20], Step [66/88], Loss: 0.0758\n",
      "Epoch [15/20], Step [67/88], Loss: 0.0569\n",
      "Epoch [15/20], Step [68/88], Loss: 0.0230\n",
      "Epoch [15/20], Step [69/88], Loss: 0.0146\n",
      "Epoch [15/20], Step [70/88], Loss: 0.0626\n",
      "Epoch [15/20], Step [71/88], Loss: 0.0644\n",
      "Epoch [15/20], Step [72/88], Loss: 0.1045\n",
      "Epoch [15/20], Step [73/88], Loss: 0.2603\n",
      "Epoch [15/20], Step [74/88], Loss: 0.0417\n",
      "Epoch [15/20], Step [75/88], Loss: 0.0530\n",
      "Epoch [15/20], Step [76/88], Loss: 0.0201\n",
      "Epoch [15/20], Step [77/88], Loss: 0.0516\n",
      "Epoch [15/20], Step [78/88], Loss: 0.1338\n",
      "Epoch [15/20], Step [79/88], Loss: 0.1040\n",
      "Epoch [15/20], Step [80/88], Loss: 0.0920\n",
      "Epoch [15/20], Step [81/88], Loss: 0.0586\n",
      "Epoch [15/20], Step [82/88], Loss: 0.0354\n",
      "Epoch [15/20], Step [83/88], Loss: 0.0555\n",
      "Epoch [15/20], Step [84/88], Loss: 0.0540\n",
      "Epoch [15/20], Step [85/88], Loss: 0.1118\n",
      "Epoch [15/20], Step [86/88], Loss: 0.0324\n",
      "Epoch [15/20], Step [87/88], Loss: 0.0771\n",
      "Epoch [15/20], Step [88/88], Loss: 0.0351\n",
      "Accuracy at 15 epoch: 0.97%\n",
      "Epoch [16/20], Step [1/88], Loss: 0.1134\n",
      "Epoch [16/20], Step [2/88], Loss: 0.0073\n",
      "Epoch [16/20], Step [3/88], Loss: 0.0811\n",
      "Epoch [16/20], Step [4/88], Loss: 0.1268\n",
      "Epoch [16/20], Step [5/88], Loss: 0.0264\n",
      "Epoch [16/20], Step [6/88], Loss: 0.0181\n",
      "Epoch [16/20], Step [7/88], Loss: 0.1496\n",
      "Epoch [16/20], Step [8/88], Loss: 0.0941\n",
      "Epoch [16/20], Step [9/88], Loss: 0.0299\n",
      "Epoch [16/20], Step [10/88], Loss: 0.1893\n",
      "Epoch [16/20], Step [11/88], Loss: 0.0709\n",
      "Epoch [16/20], Step [12/88], Loss: 0.0860\n",
      "Epoch [16/20], Step [13/88], Loss: 0.0203\n",
      "Epoch [16/20], Step [14/88], Loss: 0.1242\n",
      "Epoch [16/20], Step [15/88], Loss: 0.3369\n",
      "Epoch [16/20], Step [16/88], Loss: 0.0290\n",
      "Epoch [16/20], Step [17/88], Loss: 0.0557\n",
      "Epoch [16/20], Step [18/88], Loss: 0.0717\n",
      "Epoch [16/20], Step [19/88], Loss: 0.0339\n",
      "Epoch [16/20], Step [20/88], Loss: 0.0849\n",
      "Epoch [16/20], Step [21/88], Loss: 0.1247\n",
      "Epoch [16/20], Step [22/88], Loss: 0.0764\n",
      "Epoch [16/20], Step [23/88], Loss: 0.0075\n",
      "Epoch [16/20], Step [24/88], Loss: 0.1160\n",
      "Epoch [16/20], Step [25/88], Loss: 0.1481\n",
      "Epoch [16/20], Step [26/88], Loss: 0.0321\n",
      "Epoch [16/20], Step [27/88], Loss: 0.0971\n",
      "Epoch [16/20], Step [28/88], Loss: 0.0653\n",
      "Epoch [16/20], Step [29/88], Loss: 0.2595\n",
      "Epoch [16/20], Step [30/88], Loss: 0.0202\n",
      "Epoch [16/20], Step [31/88], Loss: 0.1395\n",
      "Epoch [16/20], Step [32/88], Loss: 0.1221\n",
      "Epoch [16/20], Step [33/88], Loss: 0.0514\n",
      "Epoch [16/20], Step [34/88], Loss: 0.0155\n",
      "Epoch [16/20], Step [35/88], Loss: 0.0505\n",
      "Epoch [16/20], Step [36/88], Loss: 0.1270\n",
      "Epoch [16/20], Step [37/88], Loss: 0.0097\n",
      "Epoch [16/20], Step [38/88], Loss: 0.0307\n",
      "Epoch [16/20], Step [39/88], Loss: 0.1106\n",
      "Epoch [16/20], Step [40/88], Loss: 0.1630\n",
      "Epoch [16/20], Step [41/88], Loss: 0.0329\n",
      "Epoch [16/20], Step [42/88], Loss: 0.0739\n",
      "Epoch [16/20], Step [43/88], Loss: 0.0488\n",
      "Epoch [16/20], Step [44/88], Loss: 0.0756\n",
      "Epoch [16/20], Step [45/88], Loss: 0.0738\n",
      "Epoch [16/20], Step [46/88], Loss: 0.1499\n",
      "Epoch [16/20], Step [47/88], Loss: 0.0496\n",
      "Epoch [16/20], Step [48/88], Loss: 0.0473\n",
      "Epoch [16/20], Step [49/88], Loss: 0.0284\n",
      "Epoch [16/20], Step [50/88], Loss: 0.1852\n",
      "Epoch [16/20], Step [51/88], Loss: 0.0559\n",
      "Epoch [16/20], Step [52/88], Loss: 0.1546\n",
      "Epoch [16/20], Step [53/88], Loss: 0.0609\n",
      "Epoch [16/20], Step [54/88], Loss: 0.0189\n",
      "Epoch [16/20], Step [55/88], Loss: 0.5569\n",
      "Epoch [16/20], Step [56/88], Loss: 0.0171\n",
      "Epoch [16/20], Step [57/88], Loss: 0.1027\n",
      "Epoch [16/20], Step [58/88], Loss: 0.0525\n",
      "Epoch [16/20], Step [59/88], Loss: 0.0334\n",
      "Epoch [16/20], Step [60/88], Loss: 0.1007\n",
      "Epoch [16/20], Step [61/88], Loss: 0.0141\n",
      "Epoch [16/20], Step [62/88], Loss: 0.0543\n",
      "Epoch [16/20], Step [63/88], Loss: 0.0482\n",
      "Epoch [16/20], Step [64/88], Loss: 0.0830\n",
      "Epoch [16/20], Step [65/88], Loss: 0.2659\n",
      "Epoch [16/20], Step [66/88], Loss: 0.0685\n",
      "Epoch [16/20], Step [67/88], Loss: 0.1162\n",
      "Epoch [16/20], Step [68/88], Loss: 0.0400\n",
      "Epoch [16/20], Step [69/88], Loss: 0.0724\n",
      "Epoch [16/20], Step [70/88], Loss: 0.0225\n",
      "Epoch [16/20], Step [71/88], Loss: 0.0602\n",
      "Epoch [16/20], Step [72/88], Loss: 0.0543\n",
      "Epoch [16/20], Step [73/88], Loss: 0.0844\n",
      "Epoch [16/20], Step [74/88], Loss: 0.0491\n",
      "Epoch [16/20], Step [75/88], Loss: 0.0571\n",
      "Epoch [16/20], Step [76/88], Loss: 0.1959\n",
      "Epoch [16/20], Step [77/88], Loss: 0.1938\n",
      "Epoch [16/20], Step [78/88], Loss: 0.0410\n",
      "Epoch [16/20], Step [79/88], Loss: 0.1750\n",
      "Epoch [16/20], Step [80/88], Loss: 0.0289\n",
      "Epoch [16/20], Step [81/88], Loss: 0.1476\n",
      "Epoch [16/20], Step [82/88], Loss: 0.1411\n",
      "Epoch [16/20], Step [83/88], Loss: 0.0263\n",
      "Epoch [16/20], Step [84/88], Loss: 0.1130\n",
      "Epoch [16/20], Step [85/88], Loss: 0.0220\n",
      "Epoch [16/20], Step [86/88], Loss: 0.0352\n",
      "Epoch [16/20], Step [87/88], Loss: 0.1084\n",
      "Epoch [16/20], Step [88/88], Loss: 0.0431\n",
      "Accuracy at 16 epoch: 0.97%\n",
      "Epoch [17/20], Step [1/88], Loss: 0.0463\n",
      "Epoch [17/20], Step [2/88], Loss: 0.0753\n",
      "Epoch [17/20], Step [3/88], Loss: 0.0059\n",
      "Epoch [17/20], Step [4/88], Loss: 0.0590\n",
      "Epoch [17/20], Step [5/88], Loss: 0.1044\n",
      "Epoch [17/20], Step [6/88], Loss: 0.0560\n",
      "Epoch [17/20], Step [7/88], Loss: 0.0698\n",
      "Epoch [17/20], Step [8/88], Loss: 0.1947\n",
      "Epoch [17/20], Step [9/88], Loss: 0.1060\n",
      "Epoch [17/20], Step [10/88], Loss: 0.0567\n",
      "Epoch [17/20], Step [11/88], Loss: 0.1520\n",
      "Epoch [17/20], Step [12/88], Loss: 0.2077\n",
      "Epoch [17/20], Step [13/88], Loss: 0.0732\n",
      "Epoch [17/20], Step [14/88], Loss: 0.0126\n",
      "Epoch [17/20], Step [15/88], Loss: 0.0213\n",
      "Epoch [17/20], Step [16/88], Loss: 0.1135\n",
      "Epoch [17/20], Step [17/88], Loss: 0.1077\n",
      "Epoch [17/20], Step [18/88], Loss: 0.1107\n",
      "Epoch [17/20], Step [19/88], Loss: 0.0055\n",
      "Epoch [17/20], Step [20/88], Loss: 0.0466\n",
      "Epoch [17/20], Step [21/88], Loss: 0.0871\n",
      "Epoch [17/20], Step [22/88], Loss: 0.1039\n",
      "Epoch [17/20], Step [23/88], Loss: 0.0483\n",
      "Epoch [17/20], Step [24/88], Loss: 0.0703\n",
      "Epoch [17/20], Step [25/88], Loss: 0.0573\n",
      "Epoch [17/20], Step [26/88], Loss: 0.1385\n",
      "Epoch [17/20], Step [27/88], Loss: 0.0288\n",
      "Epoch [17/20], Step [28/88], Loss: 0.0413\n",
      "Epoch [17/20], Step [29/88], Loss: 0.0999\n",
      "Epoch [17/20], Step [30/88], Loss: 0.0428\n",
      "Epoch [17/20], Step [31/88], Loss: 0.1184\n",
      "Epoch [17/20], Step [32/88], Loss: 0.0425\n",
      "Epoch [17/20], Step [33/88], Loss: 0.1014\n",
      "Epoch [17/20], Step [34/88], Loss: 0.0543\n",
      "Epoch [17/20], Step [35/88], Loss: 0.0198\n",
      "Epoch [17/20], Step [36/88], Loss: 0.0486\n",
      "Epoch [17/20], Step [37/88], Loss: 0.0393\n",
      "Epoch [17/20], Step [38/88], Loss: 0.0412\n",
      "Epoch [17/20], Step [39/88], Loss: 0.0685\n",
      "Epoch [17/20], Step [40/88], Loss: 0.0842\n",
      "Epoch [17/20], Step [41/88], Loss: 0.1082\n",
      "Epoch [17/20], Step [42/88], Loss: 0.0529\n",
      "Epoch [17/20], Step [43/88], Loss: 0.0953\n",
      "Epoch [17/20], Step [44/88], Loss: 0.0759\n",
      "Epoch [17/20], Step [45/88], Loss: 0.1467\n",
      "Epoch [17/20], Step [46/88], Loss: 0.0317\n",
      "Epoch [17/20], Step [47/88], Loss: 0.0946\n",
      "Epoch [17/20], Step [48/88], Loss: 0.1906\n",
      "Epoch [17/20], Step [49/88], Loss: 0.0218\n",
      "Epoch [17/20], Step [50/88], Loss: 0.0661\n",
      "Epoch [17/20], Step [51/88], Loss: 0.2326\n",
      "Epoch [17/20], Step [52/88], Loss: 0.0248\n",
      "Epoch [17/20], Step [53/88], Loss: 0.2087\n",
      "Epoch [17/20], Step [54/88], Loss: 0.0760\n",
      "Epoch [17/20], Step [55/88], Loss: 0.0524\n",
      "Epoch [17/20], Step [56/88], Loss: 0.0475\n",
      "Epoch [17/20], Step [57/88], Loss: 0.0286\n",
      "Epoch [17/20], Step [58/88], Loss: 0.1542\n",
      "Epoch [17/20], Step [59/88], Loss: 0.0592\n",
      "Epoch [17/20], Step [60/88], Loss: 0.1085\n",
      "Epoch [17/20], Step [61/88], Loss: 0.1014\n",
      "Epoch [17/20], Step [62/88], Loss: 0.1933\n",
      "Epoch [17/20], Step [63/88], Loss: 0.0212\n",
      "Epoch [17/20], Step [64/88], Loss: 0.1413\n",
      "Epoch [17/20], Step [65/88], Loss: 0.0596\n",
      "Epoch [17/20], Step [66/88], Loss: 0.0502\n",
      "Epoch [17/20], Step [67/88], Loss: 0.0502\n",
      "Epoch [17/20], Step [68/88], Loss: 0.2094\n",
      "Epoch [17/20], Step [69/88], Loss: 0.0838\n",
      "Epoch [17/20], Step [70/88], Loss: 0.0520\n",
      "Epoch [17/20], Step [71/88], Loss: 0.1826\n",
      "Epoch [17/20], Step [72/88], Loss: 0.0262\n",
      "Epoch [17/20], Step [73/88], Loss: 0.0457\n",
      "Epoch [17/20], Step [74/88], Loss: 0.1484\n",
      "Epoch [17/20], Step [75/88], Loss: 0.0545\n",
      "Epoch [17/20], Step [76/88], Loss: 0.2158\n",
      "Epoch [17/20], Step [77/88], Loss: 0.0637\n",
      "Epoch [17/20], Step [78/88], Loss: 0.0459\n",
      "Epoch [17/20], Step [79/88], Loss: 0.0548\n",
      "Epoch [17/20], Step [80/88], Loss: 0.0435\n",
      "Epoch [17/20], Step [81/88], Loss: 0.1361\n",
      "Epoch [17/20], Step [82/88], Loss: 0.1196\n",
      "Epoch [17/20], Step [83/88], Loss: 0.0227\n",
      "Epoch [17/20], Step [84/88], Loss: 0.1110\n",
      "Epoch [17/20], Step [85/88], Loss: 0.2922\n",
      "Epoch [17/20], Step [86/88], Loss: 0.1537\n",
      "Epoch [17/20], Step [87/88], Loss: 0.0385\n",
      "Epoch [17/20], Step [88/88], Loss: 0.0151\n",
      "Accuracy at 17 epoch: 0.97%\n",
      "Epoch [18/20], Step [1/88], Loss: 0.0053\n",
      "Epoch [18/20], Step [2/88], Loss: 0.0915\n",
      "Epoch [18/20], Step [3/88], Loss: 0.0168\n",
      "Epoch [18/20], Step [4/88], Loss: 0.1080\n",
      "Epoch [18/20], Step [5/88], Loss: 0.0741\n",
      "Epoch [18/20], Step [6/88], Loss: 0.0795\n",
      "Epoch [18/20], Step [7/88], Loss: 0.0668\n",
      "Epoch [18/20], Step [8/88], Loss: 0.0418\n",
      "Epoch [18/20], Step [9/88], Loss: 0.1322\n",
      "Epoch [18/20], Step [10/88], Loss: 0.0295\n",
      "Epoch [18/20], Step [11/88], Loss: 0.1107\n",
      "Epoch [18/20], Step [12/88], Loss: 0.1545\n",
      "Epoch [18/20], Step [13/88], Loss: 0.0186\n",
      "Epoch [18/20], Step [14/88], Loss: 0.0814\n",
      "Epoch [18/20], Step [15/88], Loss: 0.1102\n",
      "Epoch [18/20], Step [16/88], Loss: 0.0491\n",
      "Epoch [18/20], Step [17/88], Loss: 0.0280\n",
      "Epoch [18/20], Step [18/88], Loss: 0.0349\n",
      "Epoch [18/20], Step [19/88], Loss: 0.1014\n",
      "Epoch [18/20], Step [20/88], Loss: 0.0169\n",
      "Epoch [18/20], Step [21/88], Loss: 0.0331\n",
      "Epoch [18/20], Step [22/88], Loss: 0.0824\n",
      "Epoch [18/20], Step [23/88], Loss: 0.0312\n",
      "Epoch [18/20], Step [24/88], Loss: 0.0224\n",
      "Epoch [18/20], Step [25/88], Loss: 0.0066\n",
      "Epoch [18/20], Step [26/88], Loss: 0.0196\n",
      "Epoch [18/20], Step [27/88], Loss: 0.0467\n",
      "Epoch [18/20], Step [28/88], Loss: 0.0112\n",
      "Epoch [18/20], Step [29/88], Loss: 0.1264\n",
      "Epoch [18/20], Step [30/88], Loss: 0.0400\n",
      "Epoch [18/20], Step [31/88], Loss: 0.0202\n",
      "Epoch [18/20], Step [32/88], Loss: 0.0478\n",
      "Epoch [18/20], Step [33/88], Loss: 0.0297\n",
      "Epoch [18/20], Step [34/88], Loss: 0.0070\n",
      "Epoch [18/20], Step [35/88], Loss: 0.0105\n",
      "Epoch [18/20], Step [36/88], Loss: 0.0051\n",
      "Epoch [18/20], Step [37/88], Loss: 0.0218\n",
      "Epoch [18/20], Step [38/88], Loss: 0.1545\n",
      "Epoch [18/20], Step [39/88], Loss: 0.0322\n",
      "Epoch [18/20], Step [40/88], Loss: 0.0454\n",
      "Epoch [18/20], Step [41/88], Loss: 0.0157\n",
      "Epoch [18/20], Step [42/88], Loss: 0.0549\n",
      "Epoch [18/20], Step [43/88], Loss: 0.0693\n",
      "Epoch [18/20], Step [44/88], Loss: 0.0341\n",
      "Epoch [18/20], Step [45/88], Loss: 0.0597\n",
      "Epoch [18/20], Step [46/88], Loss: 0.1851\n",
      "Epoch [18/20], Step [47/88], Loss: 0.1143\n",
      "Epoch [18/20], Step [48/88], Loss: 0.0888\n",
      "Epoch [18/20], Step [49/88], Loss: 0.0853\n",
      "Epoch [18/20], Step [50/88], Loss: 0.0784\n",
      "Epoch [18/20], Step [51/88], Loss: 0.0341\n",
      "Epoch [18/20], Step [52/88], Loss: 0.1018\n",
      "Epoch [18/20], Step [53/88], Loss: 0.0848\n",
      "Epoch [18/20], Step [54/88], Loss: 0.0711\n",
      "Epoch [18/20], Step [55/88], Loss: 0.1427\n",
      "Epoch [18/20], Step [56/88], Loss: 0.0330\n",
      "Epoch [18/20], Step [57/88], Loss: 0.0414\n",
      "Epoch [18/20], Step [58/88], Loss: 0.0097\n",
      "Epoch [18/20], Step [59/88], Loss: 0.0807\n",
      "Epoch [18/20], Step [60/88], Loss: 0.0273\n",
      "Epoch [18/20], Step [61/88], Loss: 0.0191\n",
      "Epoch [18/20], Step [62/88], Loss: 0.0093\n",
      "Epoch [18/20], Step [63/88], Loss: 0.0696\n",
      "Epoch [18/20], Step [64/88], Loss: 0.0912\n",
      "Epoch [18/20], Step [65/88], Loss: 0.0972\n",
      "Epoch [18/20], Step [66/88], Loss: 0.0279\n",
      "Epoch [18/20], Step [67/88], Loss: 0.2340\n",
      "Epoch [18/20], Step [68/88], Loss: 0.0061\n",
      "Epoch [18/20], Step [69/88], Loss: 0.0152\n",
      "Epoch [18/20], Step [70/88], Loss: 0.1358\n",
      "Epoch [18/20], Step [71/88], Loss: 0.0906\n",
      "Epoch [18/20], Step [72/88], Loss: 0.1114\n",
      "Epoch [18/20], Step [73/88], Loss: 0.0239\n",
      "Epoch [18/20], Step [74/88], Loss: 0.0052\n",
      "Epoch [18/20], Step [75/88], Loss: 0.1874\n",
      "Epoch [18/20], Step [76/88], Loss: 0.0469\n",
      "Epoch [18/20], Step [77/88], Loss: 0.0187\n",
      "Epoch [18/20], Step [78/88], Loss: 0.0163\n",
      "Epoch [18/20], Step [79/88], Loss: 0.1283\n",
      "Epoch [18/20], Step [80/88], Loss: 0.1136\n",
      "Epoch [18/20], Step [81/88], Loss: 0.0715\n",
      "Epoch [18/20], Step [82/88], Loss: 0.0732\n",
      "Epoch [18/20], Step [83/88], Loss: 0.0601\n",
      "Epoch [18/20], Step [84/88], Loss: 0.1009\n",
      "Epoch [18/20], Step [85/88], Loss: 0.0173\n",
      "Epoch [18/20], Step [86/88], Loss: 0.0666\n",
      "Epoch [18/20], Step [87/88], Loss: 0.1052\n",
      "Epoch [18/20], Step [88/88], Loss: 0.0088\n",
      "Accuracy at 18 epoch: 0.97%\n",
      "Epoch [19/20], Step [1/88], Loss: 0.0302\n",
      "Epoch [19/20], Step [2/88], Loss: 0.0108\n",
      "Epoch [19/20], Step [3/88], Loss: 0.0904\n",
      "Epoch [19/20], Step [4/88], Loss: 0.0430\n",
      "Epoch [19/20], Step [5/88], Loss: 0.0273\n",
      "Epoch [19/20], Step [6/88], Loss: 0.0560\n",
      "Epoch [19/20], Step [7/88], Loss: 0.0240\n",
      "Epoch [19/20], Step [8/88], Loss: 0.0649\n",
      "Epoch [19/20], Step [9/88], Loss: 0.0377\n",
      "Epoch [19/20], Step [10/88], Loss: 0.0078\n",
      "Epoch [19/20], Step [11/88], Loss: 0.1330\n",
      "Epoch [19/20], Step [12/88], Loss: 0.0294\n",
      "Epoch [19/20], Step [13/88], Loss: 0.0317\n",
      "Epoch [19/20], Step [14/88], Loss: 0.0246\n",
      "Epoch [19/20], Step [15/88], Loss: 0.0217\n",
      "Epoch [19/20], Step [16/88], Loss: 0.0382\n",
      "Epoch [19/20], Step [17/88], Loss: 0.1462\n",
      "Epoch [19/20], Step [18/88], Loss: 0.0292\n",
      "Epoch [19/20], Step [19/88], Loss: 0.1407\n",
      "Epoch [19/20], Step [20/88], Loss: 0.0349\n",
      "Epoch [19/20], Step [21/88], Loss: 0.0259\n",
      "Epoch [19/20], Step [22/88], Loss: 0.0174\n",
      "Epoch [19/20], Step [23/88], Loss: 0.0079\n",
      "Epoch [19/20], Step [24/88], Loss: 0.0504\n",
      "Epoch [19/20], Step [25/88], Loss: 0.0093\n",
      "Epoch [19/20], Step [26/88], Loss: 0.0014\n",
      "Epoch [19/20], Step [27/88], Loss: 0.1212\n",
      "Epoch [19/20], Step [28/88], Loss: 0.0248\n",
      "Epoch [19/20], Step [29/88], Loss: 0.0055\n",
      "Epoch [19/20], Step [30/88], Loss: 0.0424\n",
      "Epoch [19/20], Step [31/88], Loss: 0.0566\n",
      "Epoch [19/20], Step [32/88], Loss: 0.0332\n",
      "Epoch [19/20], Step [33/88], Loss: 0.0650\n",
      "Epoch [19/20], Step [34/88], Loss: 0.0292\n",
      "Epoch [19/20], Step [35/88], Loss: 0.0167\n",
      "Epoch [19/20], Step [36/88], Loss: 0.0153\n",
      "Epoch [19/20], Step [37/88], Loss: 0.0061\n",
      "Epoch [19/20], Step [38/88], Loss: 0.0284\n",
      "Epoch [19/20], Step [39/88], Loss: 0.0256\n",
      "Epoch [19/20], Step [40/88], Loss: 0.0435\n",
      "Epoch [19/20], Step [41/88], Loss: 0.0183\n",
      "Epoch [19/20], Step [42/88], Loss: 0.1088\n",
      "Epoch [19/20], Step [43/88], Loss: 0.0443\n",
      "Epoch [19/20], Step [44/88], Loss: 0.0763\n",
      "Epoch [19/20], Step [45/88], Loss: 0.0722\n",
      "Epoch [19/20], Step [46/88], Loss: 0.0158\n",
      "Epoch [19/20], Step [47/88], Loss: 0.0264\n",
      "Epoch [19/20], Step [48/88], Loss: 0.0369\n",
      "Epoch [19/20], Step [49/88], Loss: 0.0144\n",
      "Epoch [19/20], Step [50/88], Loss: 0.0141\n",
      "Epoch [19/20], Step [51/88], Loss: 0.1003\n",
      "Epoch [19/20], Step [52/88], Loss: 0.0051\n",
      "Epoch [19/20], Step [53/88], Loss: 0.0528\n",
      "Epoch [19/20], Step [54/88], Loss: 0.1168\n",
      "Epoch [19/20], Step [55/88], Loss: 0.0620\n",
      "Epoch [19/20], Step [56/88], Loss: 0.0729\n",
      "Epoch [19/20], Step [57/88], Loss: 0.0255\n",
      "Epoch [19/20], Step [58/88], Loss: 0.0492\n",
      "Epoch [19/20], Step [59/88], Loss: 0.0590\n",
      "Epoch [19/20], Step [60/88], Loss: 0.0222\n",
      "Epoch [19/20], Step [61/88], Loss: 0.0584\n",
      "Epoch [19/20], Step [62/88], Loss: 0.0165\n",
      "Epoch [19/20], Step [63/88], Loss: 0.0196\n",
      "Epoch [19/20], Step [64/88], Loss: 0.1522\n",
      "Epoch [19/20], Step [65/88], Loss: 0.0094\n",
      "Epoch [19/20], Step [66/88], Loss: 0.1993\n",
      "Epoch [19/20], Step [67/88], Loss: 0.0275\n",
      "Epoch [19/20], Step [68/88], Loss: 0.0221\n",
      "Epoch [19/20], Step [69/88], Loss: 0.0209\n",
      "Epoch [19/20], Step [70/88], Loss: 0.0590\n",
      "Epoch [19/20], Step [71/88], Loss: 0.1034\n",
      "Epoch [19/20], Step [72/88], Loss: 0.0112\n",
      "Epoch [19/20], Step [73/88], Loss: 0.0654\n",
      "Epoch [19/20], Step [74/88], Loss: 0.0220\n",
      "Epoch [19/20], Step [75/88], Loss: 0.0680\n",
      "Epoch [19/20], Step [76/88], Loss: 0.0045\n",
      "Epoch [19/20], Step [77/88], Loss: 0.0321\n",
      "Epoch [19/20], Step [78/88], Loss: 0.0491\n",
      "Epoch [19/20], Step [79/88], Loss: 0.0455\n",
      "Epoch [19/20], Step [80/88], Loss: 0.1699\n",
      "Epoch [19/20], Step [81/88], Loss: 0.0395\n",
      "Epoch [19/20], Step [82/88], Loss: 0.0596\n",
      "Epoch [19/20], Step [83/88], Loss: 0.0593\n",
      "Epoch [19/20], Step [84/88], Loss: 0.0973\n",
      "Epoch [19/20], Step [85/88], Loss: 0.0158\n",
      "Epoch [19/20], Step [86/88], Loss: 0.0942\n",
      "Epoch [19/20], Step [87/88], Loss: 0.0006\n",
      "Epoch [19/20], Step [88/88], Loss: 0.2401\n",
      "Accuracy at 19 epoch: 0.98%\n",
      "Epoch [20/20], Step [1/88], Loss: 0.0106\n",
      "Epoch [20/20], Step [2/88], Loss: 0.0094\n",
      "Epoch [20/20], Step [3/88], Loss: 0.0662\n",
      "Epoch [20/20], Step [4/88], Loss: 0.0400\n",
      "Epoch [20/20], Step [5/88], Loss: 0.0733\n",
      "Epoch [20/20], Step [6/88], Loss: 0.0157\n",
      "Epoch [20/20], Step [7/88], Loss: 0.0562\n",
      "Epoch [20/20], Step [8/88], Loss: 0.0128\n",
      "Epoch [20/20], Step [9/88], Loss: 0.0537\n",
      "Epoch [20/20], Step [10/88], Loss: 0.0276\n",
      "Epoch [20/20], Step [11/88], Loss: 0.0242\n",
      "Epoch [20/20], Step [12/88], Loss: 0.0184\n",
      "Epoch [20/20], Step [13/88], Loss: 0.0512\n",
      "Epoch [20/20], Step [14/88], Loss: 0.0455\n",
      "Epoch [20/20], Step [15/88], Loss: 0.0319\n",
      "Epoch [20/20], Step [16/88], Loss: 0.0147\n",
      "Epoch [20/20], Step [17/88], Loss: 0.1107\n",
      "Epoch [20/20], Step [18/88], Loss: 0.3377\n",
      "Epoch [20/20], Step [19/88], Loss: 0.0363\n",
      "Epoch [20/20], Step [20/88], Loss: 0.1325\n",
      "Epoch [20/20], Step [21/88], Loss: 0.0217\n",
      "Epoch [20/20], Step [22/88], Loss: 0.0145\n",
      "Epoch [20/20], Step [23/88], Loss: 0.0115\n",
      "Epoch [20/20], Step [24/88], Loss: 0.0333\n",
      "Epoch [20/20], Step [25/88], Loss: 0.0053\n",
      "Epoch [20/20], Step [26/88], Loss: 0.0550\n",
      "Epoch [20/20], Step [27/88], Loss: 0.1546\n",
      "Epoch [20/20], Step [28/88], Loss: 0.1689\n",
      "Epoch [20/20], Step [29/88], Loss: 0.0570\n",
      "Epoch [20/20], Step [30/88], Loss: 0.0380\n",
      "Epoch [20/20], Step [31/88], Loss: 0.0496\n",
      "Epoch [20/20], Step [32/88], Loss: 0.1046\n",
      "Epoch [20/20], Step [33/88], Loss: 0.0222\n",
      "Epoch [20/20], Step [34/88], Loss: 0.0533\n",
      "Epoch [20/20], Step [35/88], Loss: 0.0389\n",
      "Epoch [20/20], Step [36/88], Loss: 0.0496\n",
      "Epoch [20/20], Step [37/88], Loss: 0.0227\n",
      "Epoch [20/20], Step [38/88], Loss: 0.0371\n",
      "Epoch [20/20], Step [39/88], Loss: 0.0214\n",
      "Epoch [20/20], Step [40/88], Loss: 0.0124\n",
      "Epoch [20/20], Step [41/88], Loss: 0.0539\n",
      "Epoch [20/20], Step [42/88], Loss: 0.1485\n",
      "Epoch [20/20], Step [43/88], Loss: 0.0225\n",
      "Epoch [20/20], Step [44/88], Loss: 0.0417\n",
      "Epoch [20/20], Step [45/88], Loss: 0.1803\n",
      "Epoch [20/20], Step [46/88], Loss: 0.0239\n",
      "Epoch [20/20], Step [47/88], Loss: 0.0162\n",
      "Epoch [20/20], Step [48/88], Loss: 0.0454\n",
      "Epoch [20/20], Step [49/88], Loss: 0.0148\n",
      "Epoch [20/20], Step [50/88], Loss: 0.2099\n",
      "Epoch [20/20], Step [51/88], Loss: 0.0521\n",
      "Epoch [20/20], Step [52/88], Loss: 0.0101\n",
      "Epoch [20/20], Step [53/88], Loss: 0.0429\n",
      "Epoch [20/20], Step [54/88], Loss: 0.0317\n",
      "Epoch [20/20], Step [55/88], Loss: 0.0020\n",
      "Epoch [20/20], Step [56/88], Loss: 0.0072\n",
      "Epoch [20/20], Step [57/88], Loss: 0.0416\n",
      "Epoch [20/20], Step [58/88], Loss: 0.0169\n",
      "Epoch [20/20], Step [59/88], Loss: 0.0483\n",
      "Epoch [20/20], Step [60/88], Loss: 0.0086\n",
      "Epoch [20/20], Step [61/88], Loss: 0.0092\n",
      "Epoch [20/20], Step [62/88], Loss: 0.0250\n",
      "Epoch [20/20], Step [63/88], Loss: 0.0085\n",
      "Epoch [20/20], Step [64/88], Loss: 0.0794\n",
      "Epoch [20/20], Step [65/88], Loss: 0.0768\n",
      "Epoch [20/20], Step [66/88], Loss: 0.0029\n",
      "Epoch [20/20], Step [67/88], Loss: 0.1602\n",
      "Epoch [20/20], Step [68/88], Loss: 0.0244\n",
      "Epoch [20/20], Step [69/88], Loss: 0.0426\n",
      "Epoch [20/20], Step [70/88], Loss: 0.0523\n",
      "Epoch [20/20], Step [71/88], Loss: 0.0344\n",
      "Epoch [20/20], Step [72/88], Loss: 0.0207\n",
      "Epoch [20/20], Step [73/88], Loss: 0.0062\n",
      "Epoch [20/20], Step [74/88], Loss: 0.0531\n",
      "Epoch [20/20], Step [75/88], Loss: 0.0135\n",
      "Epoch [20/20], Step [76/88], Loss: 0.0694\n",
      "Epoch [20/20], Step [77/88], Loss: 0.0331\n",
      "Epoch [20/20], Step [78/88], Loss: 0.0171\n",
      "Epoch [20/20], Step [79/88], Loss: 0.0332\n",
      "Epoch [20/20], Step [80/88], Loss: 0.1170\n",
      "Epoch [20/20], Step [81/88], Loss: 0.0619\n",
      "Epoch [20/20], Step [82/88], Loss: 0.1859\n",
      "Epoch [20/20], Step [83/88], Loss: 0.0237\n",
      "Epoch [20/20], Step [84/88], Loss: 0.1427\n",
      "Epoch [20/20], Step [85/88], Loss: 0.1063\n",
      "Epoch [20/20], Step [86/88], Loss: 0.0543\n",
      "Epoch [20/20], Step [87/88], Loss: 0.0104\n",
      "Epoch [20/20], Step [88/88], Loss: 0.0258\n",
      "Accuracy at 20 epoch: 0.98%\n"
     ]
    }
   ],
   "source": [
    "fit(train_data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9%. ROC-AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "score(test_data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile\n"
     ]
    }
   ],
   "source": [
    "predict(\"1.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "torch.save(model, 'AlexNet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
